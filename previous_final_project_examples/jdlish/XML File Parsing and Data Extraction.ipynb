{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This python notebook is intended to describe and lay out exactly how I collected the data I needed to do this project, and how I began to extract and process raw data into usable and organized csv files to be used in my analysis. The code below was done on my local server to make the data I collected workable and usable in python.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My data/corpus consists of Fox and CNN news transcripts obtained from Lexis Nexis. I batch requested news transcripts from Lexis Nexis (Nexis Uni) for both Fox and CNN which contain any of the following words: 'coronavirus', 'covid19', or 'pandemic'. I felt like this should enable me to obtain a large corpus of news transcripts covering the coronavirus. \n",
    "\n",
    "## My data was delivered to me from Lexis Nexus in two forms: text files where each file contains a single news transcript, and xml files where each file contains a single news transcript. Because my research question aims to assess differences between Fox and CNN coronavirus response/coverage, conducting analyses over time on both a monthly and daily level will be important. The text files I received from Lexis Nexus did not contain date information while the xml files I received did. Therefore, I had to build my corpus by extracting information from each xml file and aggregating all of it together in a neat and efficient fashion. From Lexis Nexis I received approximately 2,000 individual xml files containing CNN news transcripts, and approximately 463 individual xml files containing Fox news transcripts. As a result, my data was too large to upload to the class's server as individual xml files.\n",
    "\n",
    "## Therefore, I wrote a program to parse through each xml file, for both Fox and CNN news transcripts, and extract the news transcript text, date, and word count. I then wrote the data of each xml file I extracted to a csv file, where each row represents a different news transcript and its data. I made a different csv file for Fox and CNN, and my end result was 2 csv files containing all of the information I would need to do this project. These csv files can be found in the data folder.\n",
    "\n",
    "## I wanted to organize my data and corpus into pandas dataframes, because I have used pandas in the past, and I felt that it would be an easier way to manipulate, organize, and analyze both CNN and Fox news transcripts. This project would also entail quite a bit of data visualization, and I figured that using matplotlib on pandas dataframes would be an easier way for me to go about doing data visualization.\n",
    "\n",
    "## I have included below the code I wrote to parse through each xml file, extract the desired data, and write each news transcript's data to the aforementioned csv files. While I wasn't able to upload each individual xml file to the class's server for space and organization reasons, the code below shows how I went about extracting the data I needed from my Lexis Nexis batch request data I received on my local server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to parse through xml files and extract the desired information described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse through xml files\n",
    "def csv_row(xml_file): \n",
    "    tree = ET.parse(xml_file) \n",
    "    \n",
    "    root = tree.getroot() \n",
    "    \n",
    "    full_date = \"\"\n",
    "    body_text = \"\" \n",
    "    word_count = \"\"\n",
    "    file_name=\"\"\n",
    "    \n",
    "    if len(root[5][0][2][0])==5:\n",
    "        if root[2].text[0:10] != None:\n",
    "            full_date=root[2].text[0:10]\n",
    "        \n",
    "        else:\n",
    "            full_date=\"NA\"        \n",
    "    else:\n",
    "        full_date=\"NA\"\n",
    "        \n",
    "    a=[]\n",
    "    for elem in root[5][0][1][1][0]:\n",
    "        a.append(elem.text)\n",
    "    body_text=' '.join(a for a in a)\n",
    "    \n",
    "    word_count=root[5][0][2][1].attrib.get('number')\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_text =  \"\\\"\" + full_date + \"\\\"\" + \"|\" + \"\\\"\" + body_text + \"\\\"\" + \"|\" + \"\\\"\" + word_count + \"\\\"\" \n",
    "    \n",
    "    \n",
    "    return total_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Data Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/justinlish/Desktop/Junior Year/Comm313/jdlish-b830-s2020-04-09/_coronavirus__OR__2020_01_01_to_2020_04_09_srcMTA1MjUxNA/')\n",
    "\n",
    "\n",
    "f=[]\n",
    "for (dirpath, dirnames, filenames) in walk('.'):\n",
    "    if dirpath==\"./xml\":\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.xml') :\n",
    "                name = dirpath + \"/\" + filename \n",
    "                f.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = list(map(lambda x: csv_row(x),f))\n",
    "result_file = open('cnn_data.csv', 'w')\n",
    "for t in mapped:\n",
    "    result_file.write(t + \"\\n\")\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fox Data Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOX Data Processing\n",
    "os.chdir('/Users/justinlish/Desktop/Junior Year/Comm313/jdlish-b830-s2020-04-09/_coronavirus__OR__2020_01_01_to_2020_04_09_srcMTA2NTkxOQ/')\n",
    "\n",
    "f=[]\n",
    "for (dirpath, dirnames, filenames) in walk('.'):\n",
    "    if dirpath==\"./xml\":\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.xml') :\n",
    "                name = dirpath + \"/\" + filename \n",
    "                f.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapped = list(map(lambda x: csv_row(x),f))\n",
    "result_file = open('fox_data.csv', 'w')\n",
    "for t in mapped:\n",
    "    result_file.write(t + \"\\n\")\n",
    "result_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results of the above code produced the 2 csv files that I uploaded to the server and are called cnn_data.csv and fox_data.csv respectively. They contain all of the information this project needed from the news transcripts batch requested from Lexis Nexis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
