{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis -- Measuring Coronavirus Word Choice and Messaging Focus -- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Important Are Specific Word/Vocabulary Differences between Fox and CNN news broadcasts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Keyness Analysis, it appears that there are many words that differ significantly between the news outlets' news broadcasts containing coronavirus coverage; there are many words that CNN uses significantly more than Fox does and vice versa in each of their news broadcasts containing coronavirus coverage. It also appears that these words are important for shaping each news outlet's overall messaging and news broadcast content which are key elements of overall news outlet response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How meaningful are these vocabulary differences? Are these vocabulary differences strong enough to truly distinguish Fox and CNN news coverage about the coronavirus from one another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to answer this question I ran a logistic regression. First, I created a new dataset, taking all 463 Fox news broadcasts and taking a random sample of 463 news broadcasts from the full CNN news broadcast corpus. I then prepared the data and created a document term matrix with the most common 1000 words found between the news broadcasts. I next added a new column to the document term matrix to represent the identity of the news broadcast represented in each row (1 for CNN and 0 for Fox). With this data, I then ran a logistic regression using the 250 most common words found between the news broadcasts as explanatory variables (this was the maximum number of predictors I could use in statsmodels' logit function on this server) and the news outlet identity as the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to run a logistic regression I took the following steps. I first created and analyzed a logit model on the full dataset described above with 250 predictors, in order to find the most important words that distinguish one news outlet's coronavirus coverage from the other's. I categorized the \"most important words\" as those that ultimately had a coefficient in the resulting model with a p-value of less than 0.01, which would indicate that that word is a significant predictor of news outlet identity at the 99% confidence level. I then set these words aside, and re-ran another logit model just using this narrowed downn set of words. In this second model, I split my data up into training and test datasets, using a 70-30 split, trained the model using the training data, and then tested the resulting model on the test data. I finally analyzed the resulting predictions on the test data and compared it with the training data output in order to assess model precision and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the logit model as a whole is significant and certain words are significant predictors of news outlet identity, it will reveal that vocabulary differences between the news outlets are meaningful and important in distinguishing between their coronavirus responses. This will also show that vocabulary differences are strong enough alone to actually distinguish between Fox and CNN news coverage about the coronavirus from one another. The outcome of this logistic regression will reveal which words (if any) are the most important in distinguishing one news outlet's coverage of coronavirus versus the other's.  It will also confirm the words identified by Keyness Analysis are indeed significanlty different betweenn the news outlets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep For Logit Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Commjhub/jupyterhub/comm318_fall2019/jdlish/nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run data_processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a random subset of the CNN data equal to the Fox data length\n",
    "random_subset_cnn=data_cnn.sample(n=len(data_fox))\n",
    "random_subset_cnn2=random_subset_cnn.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep Fox Data As Well\n",
    "fox_for_pred=data_fox.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Fox and CNN Data\n",
    "combined_data=pd.concat([random_subset_cnn2,fox_for_pred],ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dist = Counter()\n",
    "\n",
    "for text in combined_data['targeted text']:\n",
    "    tokens = tokenize(text,True,strip_chars=strip_chars)\n",
    "    word_dist.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dist_sorted=sorted(word_dist, key=word_dist.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_dist_sorted[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "for text in combined_data['targeted text']:\n",
    "    row = []\n",
    "    tokens = tokenize(text,True,strip_chars=strip_chars)\n",
    "    for item in vocab:\n",
    "        row.append(item in tokens)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm=pd.DataFrame(rows,columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_outlet=[]\n",
    "for i in combined_data['News Outlet']:\n",
    "    if i==\"CNN\":\n",
    "        news_outlet.append(True)\n",
    "        \n",
    "    else:\n",
    "        news_outlet.append(False)\n",
    "    \n",
    "dtm[\"News Outlet\"]=news_outlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm=dtm.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and analyzing the first Logit model on the entire dataset to identify the most important predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.190864\n",
      "         Iterations 11\n"
     ]
    }
   ],
   "source": [
    "cols_x=dtm.columns[0:250]\n",
    "col_y=dtm.columns[1000]\n",
    "X=dtm[cols_x]\n",
    "y=dtm[col_y]\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "results_summary=result.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.725</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>    <td>News Outlet</td>         <td>AIC:</td>         <td>853.4799</td> \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-05-10 14:19</td>       <td>BIC:</td>         <td>2061.1984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>926</td>        <td>Log-Likelihood:</td>    <td>-176.74</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>249</td>           <td>LL-Null:</td>        <td>-641.85</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>676</td>         <td>LLR p-value:</td>    <td>2.8176e-79</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>11.0000</td>             <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>          <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th>  <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>the</th>             <td>3.0775</td>   <td>1.3757</td>  <td>2.2370</td>  <td>0.0253</td>  <td>0.3811</td>  <td>5.7739</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>coronavirus</th>     <td>-4.9217</td>  <td>1.4578</td>  <td>-3.3762</td> <td>0.0007</td>  <td>-7.7789</td> <td>-2.0646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>to</th>              <td>-2.0635</td>  <td>0.6650</td>  <td>-3.1032</td> <td>0.0019</td>  <td>-3.3668</td> <td>-0.7602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>of</th>              <td>2.0299</td>   <td>0.7869</td>  <td>2.5795</td>  <td>0.0099</td>  <td>0.4875</td>  <td>3.5722</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>and</th>             <td>0.4080</td>   <td>0.6314</td>  <td>0.6461</td>  <td>0.5182</td>  <td>-0.8296</td> <td>1.6456</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>a</th>               <td>-4.0957</td>  <td>0.8395</td>  <td>-4.8786</td> <td>0.0000</td>  <td>-5.7411</td> <td>-2.4502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>in</th>              <td>0.7066</td>   <td>0.6295</td>  <td>1.1225</td>  <td>0.2616</td>  <td>-0.5272</td> <td>1.9405</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>is</th>              <td>1.1130</td>   <td>0.6780</td>  <td>1.6416</td>  <td>0.1007</td>  <td>-0.2158</td> <td>2.4418</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>that</th>            <td>0.4826</td>   <td>0.5907</td>  <td>0.8170</td>  <td>0.4139</td>  <td>-0.6751</td> <td>1.6402</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>this</th>            <td>-1.3104</td>  <td>0.5959</td>  <td>-2.1991</td> <td>0.0279</td>  <td>-2.4784</td> <td>-0.1425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>for</th>             <td>1.1011</td>   <td>0.6180</td>  <td>1.7819</td>  <td>0.0748</td>  <td>-0.1100</td> <td>2.3123</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pandemic</th>        <td>0.5936</td>   <td>0.6457</td>  <td>0.9194</td>  <td>0.3579</td>  <td>-0.6719</td> <td>1.8591</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>are</th>             <td>0.0361</td>   <td>0.6442</td>  <td>0.0560</td>  <td>0.9554</td>  <td>-1.2266</td> <td>1.2987</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>on</th>              <td>-0.0346</td>  <td>0.5392</td>  <td>-0.0642</td> <td>0.9488</td>  <td>-1.0914</td> <td>1.0221</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>we</th>              <td>0.5577</td>   <td>0.6466</td>  <td>0.8625</td>  <td>0.3884</td>  <td>-0.7096</td> <td>1.8250</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>have</th>            <td>1.7652</td>   <td>0.6056</td>  <td>2.9148</td>  <td>0.0036</td>  <td>0.5782</td>  <td>2.9522</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>with</th>            <td>0.9343</td>   <td>0.5493</td>  <td>1.7009</td>  <td>0.0890</td>  <td>-0.1423</td> <td>2.0110</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>you</th>             <td>0.9192</td>   <td>0.6357</td>  <td>1.4459</td>  <td>0.1482</td>  <td>-0.3268</td> <td>2.1652</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>it</th>              <td>0.2994</td>   <td>0.6363</td>  <td>0.4705</td>  <td>0.6380</td>  <td>-0.9478</td> <td>1.5466</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>now</th>             <td>1.0338</td>   <td>0.5488</td>  <td>1.8839</td>  <td>0.0596</td>  <td>-0.0417</td> <td>2.1094</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>as</th>              <td>1.1307</td>   <td>0.5452</td>  <td>2.0739</td>  <td>0.0381</td>  <td>0.0621</td>  <td>2.1993</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>from</th>            <td>-0.1270</td>  <td>0.5346</td>  <td>-0.2376</td> <td>0.8122</td>  <td>-1.1748</td> <td>0.9208</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has</th>             <td>-0.8507</td>  <td>0.5448</td>  <td>-1.5616</td> <td>0.1184</td>  <td>-1.9184</td> <td>0.2170</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>covid19</th>         <td>-2.7965</td>  <td>0.7584</td>  <td>-3.6875</td> <td>0.0002</td>  <td>-4.2828</td> <td>-1.3101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>about</th>           <td>-0.9054</td>  <td>0.6176</td>  <td>-1.4658</td> <td>0.1427</td>  <td>-2.1159</td> <td>0.3052</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>be</th>              <td>1.2696</td>   <td>0.6441</td>  <td>1.9710</td>  <td>0.0487</td>  <td>0.0071</td>  <td>2.5320</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>they</th>            <td>0.6510</td>   <td>0.5977</td>  <td>1.0892</td>  <td>0.2761</td>  <td>-0.5205</td> <td>1.8226</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>but</th>             <td>-2.0154</td>  <td>0.6801</td>  <td>-2.9634</td> <td>0.0030</td>  <td>-3.3484</td> <td>-0.6824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>so</th>              <td>2.3191</td>   <td>0.6548</td>  <td>3.5416</td>  <td>0.0004</td>  <td>1.0357</td>  <td>3.6025</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>people</th>          <td>-0.2352</td>  <td>0.5421</td>  <td>-0.4338</td> <td>0.6644</td>  <td>-1.2977</td> <td>0.8274</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>at</th>              <td>1.0840</td>   <td>0.6443</td>  <td>1.6823</td>  <td>0.0925</td>  <td>-0.1789</td> <td>2.3468</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>i</th>               <td>0.1087</td>   <td>0.6724</td>  <td>0.1617</td>  <td>0.8715</td>  <td>-1.2091</td> <td>1.4265</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cases</th>           <td>0.8945</td>   <td>0.6408</td>  <td>1.3961</td>  <td>0.1627</td>  <td>-0.3613</td> <td>2.1504</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>new</th>             <td>0.1640</td>   <td>0.5797</td>  <td>0.2828</td>  <td>0.7773</td>  <td>-0.9722</td> <td>1.3001</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>more</th>            <td>-1.2491</td>  <td>0.6888</td>  <td>-1.8135</td> <td>0.0698</td>  <td>-2.5991</td> <td>0.1009</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>us</th>              <td>1.2259</td>   <td>0.6034</td>  <td>2.0317</td>  <td>0.0422</td>  <td>0.0433</td>  <td>2.4085</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>not</th>             <td>0.8107</td>   <td>0.6364</td>  <td>1.2739</td>  <td>0.2027</td>  <td>-0.4366</td> <td>2.0579</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>president</th>       <td>-1.2579</td>  <td>0.7376</td>  <td>-1.7054</td> <td>0.0881</td>  <td>-2.7035</td> <td>0.1878</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>who</th>             <td>1.3073</td>   <td>0.6240</td>  <td>2.0950</td>  <td>0.0362</td>  <td>0.0843</td>  <td>2.5304</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>was</th>             <td>0.3339</td>   <td>0.6362</td>  <td>0.5249</td>  <td>0.5997</td>  <td>-0.9130</td> <td>1.5808</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>all</th>             <td>-0.9794</td>  <td>0.5448</td>  <td>-1.7978</td> <td>0.0722</td>  <td>-2.0472</td> <td>0.0883</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>there</th>           <td>1.4521</td>   <td>0.5941</td>  <td>2.4441</td>  <td>0.0145</td>  <td>0.2876</td>  <td>2.6165</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>what</th>            <td>0.4771</td>   <td>0.6668</td>  <td>0.7156</td>  <td>0.4742</td>  <td>-0.8297</td> <td>1.7840</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>been</th>            <td>-0.9417</td>  <td>0.6274</td>  <td>-1.5009</td> <td>0.1334</td>  <td>-2.1714</td> <td>0.2880</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>he</th>              <td>0.0104</td>   <td>0.6949</td>  <td>0.0149</td>  <td>0.9881</td>  <td>-1.3517</td> <td>1.3724</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>will</th>            <td>-0.9417</td>  <td>0.7187</td>  <td>-1.3103</td> <td>0.1901</td>  <td>-2.3504</td> <td>0.4669</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>it's</th>            <td>-1.9122</td>  <td>0.6636</td>  <td>-2.8815</td> <td>0.0040</td>  <td>-3.2128</td> <td>-0.6115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>than</th>            <td>0.2732</td>   <td>0.7435</td>  <td>0.3674</td>  <td>0.7133</td>  <td>-1.1842</td> <td>1.7305</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>just</th>            <td>2.2409</td>   <td>0.6405</td>  <td>3.4987</td>  <td>0.0005</td>  <td>0.9855</td>  <td>3.4962</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>by</th>              <td>1.4708</td>   <td>0.7682</td>  <td>1.9146</td>  <td>0.0555</td>  <td>-0.0349</td> <td>2.9765</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>right</th>           <td>0.7949</td>   <td>0.5876</td>  <td>1.3529</td>  <td>0.1761</td>  <td>-0.3567</td> <td>1.9465</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>know</th>            <td>0.5282</td>   <td>0.6515</td>  <td>0.8108</td>  <td>0.4175</td>  <td>-0.7487</td> <td>1.8052</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>trump</th>           <td>-0.3155</td>  <td>0.8101</td>  <td>-0.3895</td> <td>0.6969</td>  <td>-1.9033</td> <td>1.2722</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>house</th>           <td>-1.7207</td>  <td>1.6097</td>  <td>-1.0690</td> <td>0.2851</td>  <td>-4.8756</td> <td>1.4342</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>if</th>              <td>-1.5232</td>  <td>0.7145</td>  <td>-2.1317</td> <td>0.0330</td>  <td>-2.9237</td> <td>-0.1227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>one</th>             <td>-0.1306</td>  <td>0.6650</td>  <td>-0.1964</td> <td>0.8443</td>  <td>-1.4339</td> <td>1.1727</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>an</th>              <td>-0.9886</td>  <td>0.6393</td>  <td>-1.5464</td> <td>0.1220</td>  <td>-2.2416</td> <td>0.2644</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>because</th>         <td>1.0777</td>   <td>0.5832</td>  <td>1.8479</td>  <td>0.0646</td>  <td>-0.0654</td> <td>2.2207</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>here</th>            <td>-1.1122</td>  <td>0.6391</td>  <td>-1.7403</td> <td>0.0818</td>  <td>-2.3647</td> <td>0.1404</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outbreak</th>        <td>2.6301</td>   <td>0.6066</td>  <td>4.3357</td>  <td>0.0000</td>  <td>1.4411</td>  <td>3.8190</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tested</th>          <td>-0.2579</td>  <td>0.8700</td>  <td>-0.2964</td> <td>0.7669</td>  <td>-1.9629</td> <td>1.4472</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>states</th>          <td>-0.9928</td>  <td>0.9956</td>  <td>-0.9972</td> <td>0.3187</td>  <td>-2.9441</td> <td>0.9585</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>or</th>              <td>-0.7809</td>  <td>0.6756</td>  <td>-1.1557</td> <td>0.2478</td>  <td>-2.1051</td> <td>0.5433</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>china</th>           <td>-1.4990</td>  <td>0.6526</td>  <td>-2.2968</td> <td>0.0216</td>  <td>-2.7781</td> <td>-0.2199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>up</th>              <td>-1.0851</td>  <td>0.6970</td>  <td>-1.5567</td> <td>0.1195</td>  <td>-2.4513</td> <td>0.2811</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>how</th>             <td>0.8503</td>   <td>0.6455</td>  <td>1.3174</td>  <td>0.1877</td>  <td>-0.4148</td> <td>2.1154</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>white</th>           <td>2.5878</td>   <td>1.6813</td>  <td>1.5392</td>  <td>0.1238</td>  <td>-0.7075</td> <td>5.8830</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>spread</th>          <td>1.5190</td>   <td>0.6554</td>  <td>2.3175</td>  <td>0.0205</td>  <td>0.2343</td>  <td>2.8036</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>patients</th>        <td>0.2931</td>   <td>0.7782</td>  <td>0.3767</td>  <td>0.7064</td>  <td>-1.2321</td> <td>1.8184</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>positive</th>        <td>0.4901</td>   <td>0.8444</td>  <td>0.5805</td>  <td>0.5616</td>  <td>-1.1648</td> <td>2.1451</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>health</th>          <td>-0.0700</td>  <td>0.6169</td>  <td>-0.1136</td> <td>0.9096</td>  <td>-1.2791</td> <td>1.1390</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>our</th>             <td>-0.6025</td>  <td>0.6797</td>  <td>-0.8865</td> <td>0.3753</td>  <td>-1.9346</td> <td>0.7296</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>response</th>        <td>-0.8788</td>  <td>0.7249</td>  <td>-1.2123</td> <td>0.2254</td>  <td>-2.2996</td> <td>0.5420</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>some</th>            <td>1.6512</td>   <td>0.6963</td>  <td>2.3715</td>  <td>0.0177</td>  <td>0.2865</td>  <td>3.0158</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>going</th>           <td>-0.1213</td>  <td>0.6187</td>  <td>-0.1961</td> <td>0.8445</td>  <td>-1.3340</td> <td>1.0913</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>well</th>            <td>-0.7277</td>  <td>0.6169</td>  <td>-1.1796</td> <td>0.2381</td>  <td>-1.9367</td> <td>0.4814</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country</th>         <td>-0.1643</td>  <td>0.6880</td>  <td>-0.2388</td> <td>0.8113</td>  <td>-1.5127</td> <td>1.1841</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>we're</th>           <td>-0.1195</td>  <td>0.6350</td>  <td>-0.1882</td> <td>0.8507</td>  <td>-1.3641</td> <td>1.1251</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>force</th>           <td>-1.1730</td>  <td>2.3805</td>  <td>-0.4928</td> <td>0.6222</td>  <td>-5.8388</td> <td>3.4927</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>task</th>            <td>3.1041</td>   <td>2.4358</td>  <td>1.2744</td>  <td>0.2025</td>  <td>-1.6700</td> <td>7.8782</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>can</th>             <td>0.4360</td>   <td>0.6329</td>  <td>0.6889</td>  <td>0.4909</td>  <td>-0.8045</td> <td>1.6766</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>first</th>           <td>0.5731</td>   <td>0.7254</td>  <td>0.7901</td>  <td>0.4295</td>  <td>-0.8487</td> <td>1.9950</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>his</th>             <td>1.5589</td>   <td>0.7013</td>  <td>2.2228</td>  <td>0.0262</td>  <td>0.1844</td>  <td>2.9334</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>out</th>             <td>0.1913</td>   <td>0.6235</td>  <td>0.3068</td>  <td>0.7590</td>  <td>-1.0307</td> <td>1.4132</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>get</th>             <td>-0.2995</td>  <td>0.6684</td>  <td>-0.4481</td> <td>0.6541</td>  <td>-1.6096</td> <td>1.0105</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>could</th>           <td>-0.9889</td>  <td>0.7306</td>  <td>-1.3536</td> <td>0.1758</td>  <td>-2.4209</td> <td>0.4430</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>united</th>          <td>0.8083</td>   <td>0.9384</td>  <td>0.8614</td>  <td>0.3890</td>  <td>-1.0309</td> <td>2.6475</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>world</th>           <td>1.0514</td>   <td>0.6938</td>  <td>1.5155</td>  <td>0.1296</td>  <td>-0.3083</td> <td>2.4111</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dr</th>              <td>1.7986</td>   <td>0.9312</td>  <td>1.9315</td>  <td>0.0534</td>  <td>-0.0265</td> <td>3.6238</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>that's</th>          <td>-0.9068</td>  <td>0.6892</td>  <td>-1.3158</td> <td>0.1883</td>  <td>-2.2576</td> <td>0.4440</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>over</th>            <td>0.0867</td>   <td>0.6546</td>  <td>0.1324</td>  <td>0.8946</td>  <td>-1.1963</td> <td>1.3697</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>their</th>           <td>0.2383</td>   <td>0.6896</td>  <td>0.3456</td>  <td>0.7297</td>  <td>-1.1133</td> <td>1.5899</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>like</th>            <td>2.4640</td>   <td>0.8223</td>  <td>2.9966</td>  <td>0.0027</td>  <td>0.8524</td>  <td>4.0756</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>do</th>              <td>1.0423</td>   <td>0.6644</td>  <td>1.5687</td>  <td>0.1167</td>  <td>-0.2600</td> <td>2.3445</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>very</th>            <td>-0.6982</td>  <td>0.7838</td>  <td>-0.8907</td> <td>0.3731</td>  <td>-2.2344</td> <td>0.8381</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>had</th>             <td>-1.2992</td>  <td>0.6776</td>  <td>-1.9173</td> <td>0.0552</td>  <td>-2.6273</td> <td>0.0289</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>number</th>          <td>0.2809</td>   <td>0.6254</td>  <td>0.4491</td>  <td>0.6533</td>  <td>-0.9449</td> <td>1.5066</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>when</th>            <td>0.5248</td>   <td>0.6915</td>  <td>0.7590</td>  <td>0.4478</td>  <td>-0.8304</td> <td>1.8801</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>today</th>           <td>-2.2635</td>  <td>0.7688</td>  <td>-2.9444</td> <td>0.0032</td>  <td>-3.7703</td> <td>-0.7568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>also</th>            <td>0.7100</td>   <td>0.7089</td>  <td>1.0016</td>  <td>0.3165</td>  <td>-0.6794</td> <td>2.0994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clip)</th>           <td>-2.6231</td>  <td>1.6384</td>  <td>-1.6010</td> <td>0.1094</td>  <td>-5.8343</td> <td>0.5882</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>video</th>           <td>2.3548</td>   <td>1.7846</td>  <td>1.3195</td>  <td>0.1870</td>  <td>-1.1429</td> <td>5.8525</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>were</th>            <td>-0.9800</td>  <td>0.6682</td>  <td>-1.4667</td> <td>0.1424</td>  <td>-2.2896</td> <td>0.3296</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tonight</th>         <td>-4.2099</td>  <td>0.9552</td>  <td>-4.4073</td> <td>0.0000</td>  <td>-6.0820</td> <td>-2.3377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state</th>           <td>1.2821</td>   <td>0.8317</td>  <td>1.5416</td>  <td>0.1232</td>  <td>-0.3480</td> <td>2.9123</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>york</th>            <td>-1.0743</td>  <td>0.9896</td>  <td>-1.0856</td> <td>0.2777</td>  <td>-3.0139</td> <td>0.8653</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>news</th>            <td>-2.6168</td>  <td>0.7503</td>  <td>-3.4876</td> <td>0.0005</td>  <td>-4.0874</td> <td>-1.1462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>after</th>           <td>-1.5032</td>  <td>0.7818</td>  <td>-1.9227</td> <td>0.0545</td>  <td>-3.0356</td> <td>0.0291</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>those</th>           <td>-1.7201</td>  <td>0.7049</td>  <td>-2.4400</td> <td>0.0147</td>  <td>-3.1017</td> <td>-0.3384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>(begin</th>          <td>-2.4197</td>  <td>1.2251</td>  <td>-1.9752</td> <td>0.0482</td>  <td>-4.8208</td> <td>-0.0186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>confirmed</th>       <td>2.4140</td>   <td>0.7817</td>  <td>3.0880</td>  <td>0.0020</td>  <td>0.8818</td>  <td>3.9462</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>americans</th>       <td>0.6483</td>   <td>0.6687</td>  <td>0.9695</td>  <td>0.3323</td>  <td>-0.6624</td> <td>1.9590</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>back</th>            <td>-1.0949</td>  <td>0.7082</td>  <td>-1.5460</td> <td>0.1221</td>  <td>-2.4829</td> <td>0.2932</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>where</th>           <td>-0.5525</td>  <td>0.6948</td>  <td>-0.7952</td> <td>0.4265</td>  <td>-1.9142</td> <td>0.8093</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>think</th>           <td>-1.9609</td>  <td>0.7580</td>  <td>-2.5871</td> <td>0.0097</td>  <td>-3.4466</td> <td>-0.4753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>being</th>           <td>0.0170</td>   <td>0.7869</td>  <td>0.0216</td>  <td>0.9828</td>  <td>-1.5253</td> <td>1.5592</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>said</th>            <td>0.8001</td>   <td>0.7507</td>  <td>1.0658</td>  <td>0.2865</td>  <td>-0.6712</td> <td>2.2713</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cnn</th>             <td>10.0868</td>  <td>1.6802</td>  <td>6.0035</td>  <td>0.0000</td>  <td>6.7938</td>  <td>13.3799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>virus</th>           <td>-0.2292</td>  <td>0.6145</td>  <td>-0.3729</td> <td>0.7092</td>  <td>-1.4336</td> <td>0.9752</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>would</th>           <td>-1.5984</td>  <td>0.7673</td>  <td>-2.0831</td> <td>0.0372</td>  <td>-3.1022</td> <td>-0.0945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>your</th>            <td>0.3811</td>   <td>0.7641</td>  <td>0.4987</td>  <td>0.6180</td>  <td>-1.1165</td> <td>1.8786</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global</th>          <td>-0.2114</td>  <td>0.7574</td>  <td>-0.2791</td> <td>0.7802</td>  <td>-1.6960</td> <td>1.2732</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>no</th>              <td>-1.1677</td>  <td>0.7585</td>  <td>-1.5395</td> <td>0.1237</td>  <td>-2.6545</td> <td>0.3190</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>much</th>            <td>-0.4884</td>  <td>0.7343</td>  <td>-0.6652</td> <td>0.5060</td>  <td>-1.9277</td> <td>0.9508</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>don't</th>           <td>-2.0235</td>  <td>0.7437</td>  <td>-2.7209</td> <td>0.0065</td>  <td>-3.4812</td> <td>-0.5659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>which</th>           <td>-1.0920</td>  <td>0.6366</td>  <td>-1.7153</td> <td>0.0863</td>  <td>-2.3398</td> <td>0.1557</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>thank</th>           <td>-3.1212</td>  <td>0.8971</td>  <td>-3.4793</td> <td>0.0005</td>  <td>-4.8795</td> <td>-1.3630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deaths</th>          <td>1.9664</td>   <td>0.8814</td>  <td>2.2311</td>  <td>0.0257</td>  <td>0.2389</td>  <td>3.6939</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>say</th>             <td>1.0513</td>   <td>0.6087</td>  <td>1.7270</td>  <td>0.0842</td>  <td>-0.1418</td> <td>2.2444</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>next</th>            <td>0.4837</td>   <td>0.6629</td>  <td>0.7297</td>  <td>0.4655</td>  <td>-0.8155</td> <td>1.7830</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>testing</th>         <td>0.3945</td>   <td>0.9172</td>  <td>0.4301</td>  <td>0.6671</td>  <td>-1.4032</td> <td>2.1922</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>says</th>            <td>-0.5393</td>  <td>0.8021</td>  <td>-0.6724</td> <td>0.5013</td>  <td>-2.1113</td> <td>1.0327</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>even</th>            <td>0.3119</td>   <td>0.6781</td>  <td>0.4599</td>  <td>0.6456</td>  <td>-1.0172</td> <td>1.6410</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>other</th>           <td>0.0128</td>   <td>0.7191</td>  <td>0.0178</td>  <td>0.9858</td>  <td>-1.3966</td> <td>1.4221</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>two</th>             <td>1.3370</td>   <td>0.7489</td>  <td>1.7853</td>  <td>0.0742</td>  <td>-0.1308</td> <td>2.8048</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>its</th>             <td>-1.5703</td>  <td>0.7827</td>  <td>-2.0062</td> <td>0.0448</td>  <td>-3.1044</td> <td>-0.0362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>them</th>            <td>-1.5160</td>  <td>0.7768</td>  <td>-1.9516</td> <td>0.0510</td>  <td>-3.0385</td> <td>0.0065</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>death</th>           <td>-1.5653</td>  <td>0.8734</td>  <td>-1.7923</td> <td>0.0731</td>  <td>-3.2770</td> <td>0.1464</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>many</th>            <td>0.5979</td>   <td>0.8657</td>  <td>0.6906</td>  <td>0.4898</td>  <td>-1.0989</td> <td>2.2947</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>test</th>            <td>2.0124</td>   <td>1.0148</td>  <td>1.9830</td>  <td>0.0474</td>  <td>0.0233</td>  <td>4.0015</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>they're</th>         <td>-1.3271</td>  <td>0.8088</td>  <td>-1.6407</td> <td>0.1009</td>  <td>-2.9124</td> <td>0.2582</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>into</th>            <td>-0.2564</td>  <td>0.7734</td>  <td>-0.3316</td> <td>0.7402</td>  <td>-1.7724</td> <td>1.2595</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>these</th>           <td>-1.5811</td>  <td>0.8629</td>  <td>-1.8323</td> <td>0.0669</td>  <td>-3.2723</td> <td>0.1101</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>really</th>          <td>1.5791</td>   <td>0.7964</td>  <td>1.9827</td>  <td>0.0474</td>  <td>0.0181</td>  <td>3.1401</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time</th>            <td>-0.2017</td>  <td>0.8565</td>  <td>-0.2355</td> <td>0.8138</td>  <td>-1.8804</td> <td>1.4769</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>see</th>             <td>1.4138</td>   <td>0.7572</td>  <td>1.8671</td>  <td>0.0619</td>  <td>-0.0703</td> <td>2.8978</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>coming</th>          <td>-1.8303</td>  <td>0.9338</td>  <td>-1.9601</td> <td>0.0500</td>  <td>-3.6604</td> <td>-0.0001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>around</th>          <td>-2.7941</td>  <td>0.9457</td>  <td>-2.9547</td> <td>0.0031</td>  <td>-4.6476</td> <td>-0.9407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>most</th>            <td>-0.4292</td>  <td>0.7981</td>  <td>-0.5377</td> <td>0.5908</td>  <td>-1.9935</td> <td>1.1352</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>day</th>             <td>0.4503</td>   <td>0.7510</td>  <td>0.5996</td>  <td>0.5488</td>  <td>-1.0217</td> <td>1.9224</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>crisis</th>          <td>0.9709</td>   <td>0.7906</td>  <td>1.2281</td>  <td>0.2194</td>  <td>-0.5786</td> <td>2.5203</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>during</th>          <td>0.8097</td>   <td>0.8435</td>  <td>0.9599</td>  <td>0.3371</td>  <td>-0.8435</td> <td>2.4630</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>officials</th>       <td>1.0916</td>   <td>0.6926</td>  <td>1.5761</td>  <td>0.1150</td>  <td>-0.2658</td> <td>2.4491</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>still</th>           <td>0.1479</td>   <td>0.8569</td>  <td>0.1725</td>  <td>0.8630</td>  <td>-1.5316</td> <td>1.8273</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>take</th>            <td>-0.0272</td>  <td>0.7845</td>  <td>-0.0346</td> <td>0.9724</td>  <td>-1.5647</td> <td>1.5104</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>want</th>            <td>-0.6253</td>  <td>0.8417</td>  <td>-0.7428</td> <td>0.4576</td>  <td>-2.2750</td> <td>1.0245</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>died</th>            <td>1.3241</td>   <td>0.9362</td>  <td>1.4143</td>  <td>0.1573</td>  <td>-0.5108</td> <td>3.1589</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>flu</th>             <td>-0.6942</td>  <td>0.8118</td>  <td>-0.8552</td> <td>0.3925</td>  <td>-2.2853</td> <td>0.8968</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>i'm</th>             <td>-2.4063</td>  <td>0.7986</td>  <td>-3.0131</td> <td>0.0026</td>  <td>-3.9716</td> <td>-0.8411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fight</th>           <td>0.7070</td>   <td>0.8937</td>  <td>0.7910</td>  <td>0.4289</td>  <td>-1.0447</td> <td>2.4586</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>there's</th>         <td>0.9278</td>   <td>0.7124</td>  <td>1.3025</td>  <td>0.1928</td>  <td>-0.4684</td> <td>2.3240</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>need</th>            <td>-0.9397</td>  <td>0.8207</td>  <td>-1.1451</td> <td>0.2522</td>  <td>-2.5483</td> <td>0.6688</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>medical</th>         <td>0.3261</td>   <td>0.9432</td>  <td>0.3458</td>  <td>0.7295</td>  <td>-1.5224</td> <td>2.1747</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>against</th>         <td>-1.1636</td>  <td>0.9322</td>  <td>-1.2481</td> <td>0.2120</td>  <td>-2.9907</td> <td>0.6636</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>any</th>             <td>-1.5043</td>  <td>0.8132</td>  <td>-1.8499</td> <td>0.0643</td>  <td>-3.0980</td> <td>0.0895</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>case</th>            <td>-1.1788</td>  <td>0.7650</td>  <td>-1.5409</td> <td>0.1233</td>  <td>-2.6781</td> <td>0.3206</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>should</th>          <td>-0.1815</td>  <td>0.7988</td>  <td>-0.2273</td> <td>0.8202</td>  <td>-1.7472</td> <td>1.3841</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chinese</th>         <td>-1.5662</td>  <td>0.9433</td>  <td>-1.6602</td> <td>0.0969</td>  <td>-3.4151</td> <td>0.2828</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>briefing</th>        <td>-1.8140</td>  <td>1.0822</td>  <td>-1.6762</td> <td>0.0937</td>  <td>-3.9352</td> <td>0.3071</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>last</th>            <td>-0.2482</td>  <td>0.7507</td>  <td>-0.3306</td> <td>0.7410</td>  <td>-1.7195</td> <td>1.2231</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>may</th>             <td>2.2439</td>   <td>1.0174</td>  <td>2.2055</td>  <td>0.0274</td>  <td>0.2498</td>  <td>4.2380</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hospital</th>        <td>-1.4056</td>  <td>0.9674</td>  <td>-1.4530</td> <td>0.1462</td>  <td>-3.3017</td> <td>0.4905</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>did</th>             <td>1.3088</td>   <td>0.9016</td>  <td>1.4517</td>  <td>0.1466</td>  <td>-0.4582</td> <td>3.0758</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ship</th>            <td>4.9637</td>   <td>1.1551</td>  <td>4.2970</td>  <td>0.0000</td>  <td>2.6996</td>  <td>7.2277</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>me</th>              <td>0.2088</td>   <td>0.7209</td>  <td>0.2896</td>  <td>0.7721</td>  <td>-1.2041</td> <td>1.6217</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yes</th>             <td>-0.7332</td>  <td>0.7613</td>  <td>-0.9631</td> <td>0.3355</td>  <td>-2.2254</td> <td>0.7590</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>look</th>            <td>-0.1710</td>  <td>0.8315</td>  <td>-0.2056</td> <td>0.8371</td>  <td>-1.8008</td> <td>1.4588</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>help</th>            <td>0.4563</td>   <td>0.9264</td>  <td>0.4926</td>  <td>0.6223</td>  <td>-1.3594</td> <td>2.2721</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>then</th>            <td>-1.4406</td>  <td>1.0218</td>  <td>-1.4099</td> <td>0.1586</td>  <td>-3.4433</td> <td>0.5620</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>we'll</th>           <td>-0.2098</td>  <td>0.9204</td>  <td>-0.2279</td> <td>0.8197</td>  <td>-2.0138</td> <td>1.5942</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>go</th>              <td>1.4289</td>   <td>0.7709</td>  <td>1.8536</td>  <td>0.0638</td>  <td>-0.0820</td> <td>2.9398</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>across</th>          <td>2.0072</td>   <td>0.9118</td>  <td>2.2013</td>  <td>0.0277</td>  <td>0.2201</td>  <td>3.7943</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>american</th>        <td>-2.2889</td>  <td>1.0802</td>  <td>-2.1190</td> <td>0.0341</td>  <td>-4.4060</td> <td>-0.1718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lot</th>             <td>0.3169</td>   <td>0.9579</td>  <td>0.3308</td>  <td>0.7408</td>  <td>-1.5606</td> <td>2.1944</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>every</th>           <td>0.4041</td>   <td>0.8954</td>  <td>0.4513</td>  <td>0.6517</td>  <td>-1.3508</td> <td>2.1590</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>another</th>         <td>-0.2425</td>  <td>0.8694</td>  <td>-0.2789</td> <td>0.7803</td>  <td>-1.9465</td> <td>1.4615</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>percent</th>         <td>-0.0324</td>  <td>0.9230</td>  <td>-0.0351</td> <td>0.9720</td>  <td>-1.8415</td> <td>1.7767</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>down</th>            <td>1.1101</td>   <td>0.8898</td>  <td>1.2476</td>  <td>0.2122</td>  <td>-0.6338</td> <td>2.8541</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>economy</th>         <td>0.2617</td>   <td>0.7756</td>  <td>0.3374</td>  <td>0.7358</td>  <td>-1.2585</td> <td>1.7818</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>week</th>            <td>0.8710</td>   <td>0.9695</td>  <td>0.8983</td>  <td>0.3690</td>  <td>-1.0293</td> <td>2.7712</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>symptoms</th>        <td>1.9041</td>   <td>0.8200</td>  <td>2.3222</td>  <td>0.0202</td>  <td>0.2970</td>  <td>3.5112</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>live</th>            <td>1.0588</td>   <td>0.8917</td>  <td>1.1874</td>  <td>0.2351</td>  <td>-0.6889</td> <td>2.8065</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>care</th>            <td>2.3414</td>   <td>1.0134</td>  <td>2.3105</td>  <td>0.0209</td>  <td>0.3552</td>  <td>4.3275</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>through</th>         <td>3.8890</td>   <td>1.0662</td>  <td>3.6477</td>  <td>0.0003</td>  <td>1.7994</td>  <td>5.9786</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>way</th>             <td>-3.4729</td>  <td>0.8984</td>  <td>-3.8656</td> <td>0.0001</td>  <td>-5.2337</td> <td>-1.7120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>america</th>         <td>-2.0378</td>  <td>1.1439</td>  <td>-1.7814</td> <td>0.0748</td>  <td>-4.2798</td> <td>0.2042</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>make</th>            <td>-0.4007</td>  <td>0.8738</td>  <td>-0.4586</td> <td>0.6466</td>  <td>-2.1134</td> <td>1.3120</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>correspondent</th>   <td>-4.3753</td>  <td>1.1735</td>  <td>-3.7284</td> <td>0.0002</td>  <td>-6.6753</td> <td>-2.0753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>home</th>            <td>1.1532</td>   <td>0.8235</td>  <td>1.4004</td>  <td>0.1614</td>  <td>-0.4608</td> <td>2.7672</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>you're</th>          <td>0.2073</td>   <td>0.8518</td>  <td>0.2434</td>  <td>0.8077</td>  <td>-1.4621</td> <td>1.8768</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>city</th>            <td>4.4666</td>   <td>1.0847</td>  <td>4.1178</td>  <td>0.0000</td>  <td>2.3407</td>  <td>6.5926</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>before</th>          <td>-2.1507</td>  <td>0.9756</td>  <td>-2.2046</td> <td>0.0275</td>  <td>-4.0628</td> <td>-0.2387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>(end</th>            <td>-2.0658</td>  <td>1.1027</td>  <td>-1.8735</td> <td>0.0610</td>  <td>-4.2270</td> <td>0.0953</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>infected</th>        <td>0.6004</td>   <td>0.8727</td>  <td>0.6879</td>  <td>0.4915</td>  <td>-1.1102</td> <td>2.3109</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>saying</th>          <td>-0.9341</td>  <td>1.0460</td>  <td>-0.8930</td> <td>0.3718</td>  <td>-2.9842</td> <td>1.1160</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>birx</th>            <td>9.7789</td>   <td>3.9925</td>  <td>2.4493</td>  <td>0.0143</td>  <td>1.9538</td>  <td>17.6040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>getting</th>         <td>2.0507</td>   <td>0.8503</td>  <td>2.4117</td>  <td>0.0159</td>  <td>0.3841</td>  <td>3.7172</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>novel</th>           <td>2.4561</td>   <td>1.0152</td>  <td>2.4195</td>  <td>0.0155</td>  <td>0.4665</td>  <td>4.4458</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>days</th>            <td>2.9489</td>   <td>0.9608</td>  <td>3.0691</td>  <td>0.0021</td>  <td>1.0657</td>  <td>4.8321</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>impact</th>          <td>0.4329</td>   <td>0.9159</td>  <td>0.4726</td>  <td>0.6365</td>  <td>-1.3623</td> <td>2.2281</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>government</th>      <td>-0.8644</td>  <td>0.8755</td>  <td>-0.9874</td> <td>0.3234</td>  <td>-2.5803</td> <td>0.8514</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>only</th>            <td>-0.9158</td>  <td>0.7858</td>  <td>-1.1655</td> <td>0.2438</td>  <td>-2.4559</td> <td>0.6243</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>he's</th>            <td>-0.2846</td>  <td>0.9139</td>  <td>-0.3114</td> <td>0.7555</td>  <td>-2.0758</td> <td>1.5066</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>got</th>             <td>-0.4498</td>  <td>0.8202</td>  <td>-0.5484</td> <td>0.5834</td>  <td>-2.0573</td> <td>1.1577</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emergency</th>       <td>0.6346</td>   <td>0.8467</td>  <td>0.7495</td>  <td>0.4536</td>  <td>-1.0248</td> <td>2.2940</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wuhan</th>           <td>-1.1144</td>  <td>0.9045</td>  <td>-1.2320</td> <td>0.2179</td>  <td>-2.8871</td> <td>0.6584</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bill</th>            <td>-5.1435</td>  <td>1.3419</td>  <td>-3.8331</td> <td>0.0001</td>  <td>-7.7735</td> <td>-2.5135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>she</th>             <td>2.4538</td>   <td>1.1491</td>  <td>2.1355</td>  <td>0.0327</td>  <td>0.2017</td>  <td>4.7059</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>washington</th>      <td>1.2737</td>   <td>1.0479</td>  <td>1.2155</td>  <td>0.2242</td>  <td>-0.7801</td> <td>3.3276</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fears</th>           <td>2.5457</td>   <td>0.8801</td>  <td>2.8926</td>  <td>0.0038</td>  <td>0.8208</td>  <td>4.2706</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>come</th>            <td>-0.4964</td>  <td>0.9453</td>  <td>-0.5251</td> <td>0.5995</td>  <td>-2.3491</td> <td>1.3563</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>economic</th>        <td>0.0328</td>   <td>0.7897</td>  <td>0.0416</td>  <td>0.9668</td>  <td>-1.5149</td> <td>1.5805</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>my</th>              <td>0.9748</td>   <td>0.9203</td>  <td>1.0592</td>  <td>0.2895</td>  <td>-0.8290</td> <td>2.7785</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>toll</th>            <td>1.9993</td>   <td>1.2408</td>  <td>1.6113</td>  <td>0.1071</td>  <td>-0.4326</td> <td>4.4312</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deborah</th>        <td>-11.3658</td>  <td>4.1021</td>  <td>-2.7707</td> <td>0.0056</td> <td>-19.4057</td> <td>-3.3259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>why</th>             <td>0.2776</td>   <td>0.8896</td>  <td>0.3121</td>  <td>0.7550</td>  <td>-1.4659</td> <td>2.0211</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>patient</th>         <td>1.3549</td>   <td>0.8839</td>  <td>1.5329</td>  <td>0.1253</td>  <td>-0.3775</td> <td>3.0873</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>under</th>           <td>1.3712</td>   <td>0.8634</td>  <td>1.5882</td>  <td>0.1122</td>  <td>-0.3210</td> <td>3.0635</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>far</th>             <td>-1.5733</td>  <td>0.9724</td>  <td>-1.6180</td> <td>0.1057</td>  <td>-3.4792</td> <td>0.3325</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>course</th>          <td>1.3398</td>   <td>0.8542</td>  <td>1.5685</td>  <td>0.1168</td>  <td>-0.3344</td> <td>3.0141</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>again</th>           <td>-2.7035</td>  <td>0.9334</td>  <td>-2.8963</td> <td>0.0038</td>  <td>-4.5329</td> <td>-0.8740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>spreading</th>       <td>-0.1063</td>  <td>0.9374</td>  <td>-0.1134</td> <td>0.9097</td>  <td>-1.9436</td> <td>1.7311</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>least</th>           <td>1.6866</td>   <td>1.1584</td>  <td>1.4560</td>  <td>0.1454</td>  <td>-0.5838</td> <td>3.9569</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>already</th>         <td>1.2903</td>   <td>0.9347</td>  <td>1.3804</td>  <td>0.1675</td>  <td>-0.5417</td> <td>3.1224</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>public</th>          <td>-0.8873</td>  <td>0.9796</td>  <td>-0.9058</td> <td>0.3650</td>  <td>-2.8074</td> <td>1.0327</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>latest</th>          <td>-0.8115</td>  <td>0.8483</td>  <td>-0.9566</td> <td>0.3388</td>  <td>-2.4742</td> <td>0.8512</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>good</th>            <td>-2.0162</td>  <td>0.8973</td>  <td>-2.2470</td> <td>0.0246</td>  <td>-3.7748</td> <td>-0.2576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>actually</th>        <td>-0.4664</td>  <td>1.0521</td>  <td>-0.4433</td> <td>0.6576</td>  <td>-2.5284</td> <td>1.5957</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hospitals</th>       <td>2.9452</td>   <td>1.1303</td>  <td>2.6057</td>  <td>0.0092</td>  <td>0.7299</td>  <td>5.1605</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weeks</th>           <td>2.4802</td>   <td>0.9355</td>  <td>2.6513</td>  <td>0.0080</td>  <td>0.6467</td>  <td>4.3137</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>comes</th>           <td>-2.6700</td>  <td>1.0550</td>  <td>-2.5307</td> <td>0.0114</td>  <td>-4.7378</td> <td>-0.6021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>administration</th>  <td>0.1036</td>   <td>0.9488</td>  <td>0.1092</td>  <td>0.9130</td>  <td>-1.7560</td> <td>1.9632</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>italy</th>           <td>-1.3920</td>  <td>0.9896</td>  <td>-1.4066</td> <td>0.1595</td>  <td>-3.3316</td> <td>0.5476</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>risk</th>            <td>-1.4968</td>  <td>1.0598</td>  <td>-1.4123</td> <td>0.1579</td>  <td>-3.5740</td> <td>0.5804</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vice</th>            <td>-1.8051</td>  <td>1.0610</td>  <td>-1.7013</td> <td>0.0889</td>  <td>-3.8846</td> <td>0.2744</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>point</th>           <td>-0.7907</td>  <td>0.9407</td>  <td>-0.8406</td> <td>0.4006</td>  <td>-2.6344</td> <td>1.0529</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>things</th>          <td>-1.7684</td>  <td>0.8735</td>  <td>-2.0246</td> <td>0.0429</td>  <td>-3.4804</td> <td>-0.0564</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>disease</th>         <td>-1.0920</td>  <td>0.8845</td>  <td>-1.2347</td> <td>0.2169</td>  <td>-2.8255</td> <td>0.6415</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>big</th>             <td>-3.7365</td>  <td>0.8741</td>  <td>-4.2749</td> <td>0.0000</td>  <td>-5.4496</td> <td>-2.0233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>doing</th>           <td>-4.2863</td>  <td>1.0895</td>  <td>-3.9341</td> <td>0.0001</td>  <td>-6.4217</td> <td>-2.1509</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                         Results: Logit\n",
       "=================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.725     \n",
       "Dependent Variable: News Outlet      AIC:              853.4799  \n",
       "Date:               2020-05-10 14:19 BIC:              2061.1984 \n",
       "No. Observations:   926              Log-Likelihood:   -176.74   \n",
       "Df Model:           249              LL-Null:          -641.85   \n",
       "Df Residuals:       676              LLR p-value:      2.8176e-79\n",
       "Converged:          1.0000           Scale:            1.0000    \n",
       "No. Iterations:     11.0000                                      \n",
       "-----------------------------------------------------------------\n",
       "                 Coef.   Std.Err.    z    P>|z|   [0.025   0.975]\n",
       "-----------------------------------------------------------------\n",
       "the               3.0775   1.3757  2.2370 0.0253   0.3811  5.7739\n",
       "coronavirus      -4.9217   1.4578 -3.3762 0.0007  -7.7789 -2.0646\n",
       "to               -2.0635   0.6650 -3.1032 0.0019  -3.3668 -0.7602\n",
       "of                2.0299   0.7869  2.5795 0.0099   0.4875  3.5722\n",
       "and               0.4080   0.6314  0.6461 0.5182  -0.8296  1.6456\n",
       "a                -4.0957   0.8395 -4.8786 0.0000  -5.7411 -2.4502\n",
       "in                0.7066   0.6295  1.1225 0.2616  -0.5272  1.9405\n",
       "is                1.1130   0.6780  1.6416 0.1007  -0.2158  2.4418\n",
       "that              0.4826   0.5907  0.8170 0.4139  -0.6751  1.6402\n",
       "this             -1.3104   0.5959 -2.1991 0.0279  -2.4784 -0.1425\n",
       "for               1.1011   0.6180  1.7819 0.0748  -0.1100  2.3123\n",
       "pandemic          0.5936   0.6457  0.9194 0.3579  -0.6719  1.8591\n",
       "are               0.0361   0.6442  0.0560 0.9554  -1.2266  1.2987\n",
       "on               -0.0346   0.5392 -0.0642 0.9488  -1.0914  1.0221\n",
       "we                0.5577   0.6466  0.8625 0.3884  -0.7096  1.8250\n",
       "have              1.7652   0.6056  2.9148 0.0036   0.5782  2.9522\n",
       "with              0.9343   0.5493  1.7009 0.0890  -0.1423  2.0110\n",
       "you               0.9192   0.6357  1.4459 0.1482  -0.3268  2.1652\n",
       "it                0.2994   0.6363  0.4705 0.6380  -0.9478  1.5466\n",
       "now               1.0338   0.5488  1.8839 0.0596  -0.0417  2.1094\n",
       "as                1.1307   0.5452  2.0739 0.0381   0.0621  2.1993\n",
       "from             -0.1270   0.5346 -0.2376 0.8122  -1.1748  0.9208\n",
       "has              -0.8507   0.5448 -1.5616 0.1184  -1.9184  0.2170\n",
       "covid19          -2.7965   0.7584 -3.6875 0.0002  -4.2828 -1.3101\n",
       "about            -0.9054   0.6176 -1.4658 0.1427  -2.1159  0.3052\n",
       "be                1.2696   0.6441  1.9710 0.0487   0.0071  2.5320\n",
       "they              0.6510   0.5977  1.0892 0.2761  -0.5205  1.8226\n",
       "but              -2.0154   0.6801 -2.9634 0.0030  -3.3484 -0.6824\n",
       "so                2.3191   0.6548  3.5416 0.0004   1.0357  3.6025\n",
       "people           -0.2352   0.5421 -0.4338 0.6644  -1.2977  0.8274\n",
       "at                1.0840   0.6443  1.6823 0.0925  -0.1789  2.3468\n",
       "i                 0.1087   0.6724  0.1617 0.8715  -1.2091  1.4265\n",
       "cases             0.8945   0.6408  1.3961 0.1627  -0.3613  2.1504\n",
       "new               0.1640   0.5797  0.2828 0.7773  -0.9722  1.3001\n",
       "more             -1.2491   0.6888 -1.8135 0.0698  -2.5991  0.1009\n",
       "us                1.2259   0.6034  2.0317 0.0422   0.0433  2.4085\n",
       "not               0.8107   0.6364  1.2739 0.2027  -0.4366  2.0579\n",
       "president        -1.2579   0.7376 -1.7054 0.0881  -2.7035  0.1878\n",
       "who               1.3073   0.6240  2.0950 0.0362   0.0843  2.5304\n",
       "was               0.3339   0.6362  0.5249 0.5997  -0.9130  1.5808\n",
       "all              -0.9794   0.5448 -1.7978 0.0722  -2.0472  0.0883\n",
       "there             1.4521   0.5941  2.4441 0.0145   0.2876  2.6165\n",
       "what              0.4771   0.6668  0.7156 0.4742  -0.8297  1.7840\n",
       "been             -0.9417   0.6274 -1.5009 0.1334  -2.1714  0.2880\n",
       "he                0.0104   0.6949  0.0149 0.9881  -1.3517  1.3724\n",
       "will             -0.9417   0.7187 -1.3103 0.1901  -2.3504  0.4669\n",
       "it's             -1.9122   0.6636 -2.8815 0.0040  -3.2128 -0.6115\n",
       "than              0.2732   0.7435  0.3674 0.7133  -1.1842  1.7305\n",
       "just              2.2409   0.6405  3.4987 0.0005   0.9855  3.4962\n",
       "by                1.4708   0.7682  1.9146 0.0555  -0.0349  2.9765\n",
       "right             0.7949   0.5876  1.3529 0.1761  -0.3567  1.9465\n",
       "know              0.5282   0.6515  0.8108 0.4175  -0.7487  1.8052\n",
       "trump            -0.3155   0.8101 -0.3895 0.6969  -1.9033  1.2722\n",
       "house            -1.7207   1.6097 -1.0690 0.2851  -4.8756  1.4342\n",
       "if               -1.5232   0.7145 -2.1317 0.0330  -2.9237 -0.1227\n",
       "one              -0.1306   0.6650 -0.1964 0.8443  -1.4339  1.1727\n",
       "an               -0.9886   0.6393 -1.5464 0.1220  -2.2416  0.2644\n",
       "because           1.0777   0.5832  1.8479 0.0646  -0.0654  2.2207\n",
       "here             -1.1122   0.6391 -1.7403 0.0818  -2.3647  0.1404\n",
       "outbreak          2.6301   0.6066  4.3357 0.0000   1.4411  3.8190\n",
       "tested           -0.2579   0.8700 -0.2964 0.7669  -1.9629  1.4472\n",
       "states           -0.9928   0.9956 -0.9972 0.3187  -2.9441  0.9585\n",
       "or               -0.7809   0.6756 -1.1557 0.2478  -2.1051  0.5433\n",
       "china            -1.4990   0.6526 -2.2968 0.0216  -2.7781 -0.2199\n",
       "up               -1.0851   0.6970 -1.5567 0.1195  -2.4513  0.2811\n",
       "how               0.8503   0.6455  1.3174 0.1877  -0.4148  2.1154\n",
       "white             2.5878   1.6813  1.5392 0.1238  -0.7075  5.8830\n",
       "spread            1.5190   0.6554  2.3175 0.0205   0.2343  2.8036\n",
       "patients          0.2931   0.7782  0.3767 0.7064  -1.2321  1.8184\n",
       "positive          0.4901   0.8444  0.5805 0.5616  -1.1648  2.1451\n",
       "health           -0.0700   0.6169 -0.1136 0.9096  -1.2791  1.1390\n",
       "our              -0.6025   0.6797 -0.8865 0.3753  -1.9346  0.7296\n",
       "response         -0.8788   0.7249 -1.2123 0.2254  -2.2996  0.5420\n",
       "some              1.6512   0.6963  2.3715 0.0177   0.2865  3.0158\n",
       "going            -0.1213   0.6187 -0.1961 0.8445  -1.3340  1.0913\n",
       "well             -0.7277   0.6169 -1.1796 0.2381  -1.9367  0.4814\n",
       "country          -0.1643   0.6880 -0.2388 0.8113  -1.5127  1.1841\n",
       "we're            -0.1195   0.6350 -0.1882 0.8507  -1.3641  1.1251\n",
       "force            -1.1730   2.3805 -0.4928 0.6222  -5.8388  3.4927\n",
       "task              3.1041   2.4358  1.2744 0.2025  -1.6700  7.8782\n",
       "can               0.4360   0.6329  0.6889 0.4909  -0.8045  1.6766\n",
       "first             0.5731   0.7254  0.7901 0.4295  -0.8487  1.9950\n",
       "his               1.5589   0.7013  2.2228 0.0262   0.1844  2.9334\n",
       "out               0.1913   0.6235  0.3068 0.7590  -1.0307  1.4132\n",
       "get              -0.2995   0.6684 -0.4481 0.6541  -1.6096  1.0105\n",
       "could            -0.9889   0.7306 -1.3536 0.1758  -2.4209  0.4430\n",
       "united            0.8083   0.9384  0.8614 0.3890  -1.0309  2.6475\n",
       "world             1.0514   0.6938  1.5155 0.1296  -0.3083  2.4111\n",
       "dr                1.7986   0.9312  1.9315 0.0534  -0.0265  3.6238\n",
       "that's           -0.9068   0.6892 -1.3158 0.1883  -2.2576  0.4440\n",
       "over              0.0867   0.6546  0.1324 0.8946  -1.1963  1.3697\n",
       "their             0.2383   0.6896  0.3456 0.7297  -1.1133  1.5899\n",
       "like              2.4640   0.8223  2.9966 0.0027   0.8524  4.0756\n",
       "do                1.0423   0.6644  1.5687 0.1167  -0.2600  2.3445\n",
       "very             -0.6982   0.7838 -0.8907 0.3731  -2.2344  0.8381\n",
       "had              -1.2992   0.6776 -1.9173 0.0552  -2.6273  0.0289\n",
       "number            0.2809   0.6254  0.4491 0.6533  -0.9449  1.5066\n",
       "when              0.5248   0.6915  0.7590 0.4478  -0.8304  1.8801\n",
       "today            -2.2635   0.7688 -2.9444 0.0032  -3.7703 -0.7568\n",
       "also              0.7100   0.7089  1.0016 0.3165  -0.6794  2.0994\n",
       "clip)            -2.6231   1.6384 -1.6010 0.1094  -5.8343  0.5882\n",
       "video             2.3548   1.7846  1.3195 0.1870  -1.1429  5.8525\n",
       "were             -0.9800   0.6682 -1.4667 0.1424  -2.2896  0.3296\n",
       "tonight          -4.2099   0.9552 -4.4073 0.0000  -6.0820 -2.3377\n",
       "state             1.2821   0.8317  1.5416 0.1232  -0.3480  2.9123\n",
       "york             -1.0743   0.9896 -1.0856 0.2777  -3.0139  0.8653\n",
       "news             -2.6168   0.7503 -3.4876 0.0005  -4.0874 -1.1462\n",
       "after            -1.5032   0.7818 -1.9227 0.0545  -3.0356  0.0291\n",
       "those            -1.7201   0.7049 -2.4400 0.0147  -3.1017 -0.3384\n",
       "(begin           -2.4197   1.2251 -1.9752 0.0482  -4.8208 -0.0186\n",
       "confirmed         2.4140   0.7817  3.0880 0.0020   0.8818  3.9462\n",
       "americans         0.6483   0.6687  0.9695 0.3323  -0.6624  1.9590\n",
       "back             -1.0949   0.7082 -1.5460 0.1221  -2.4829  0.2932\n",
       "where            -0.5525   0.6948 -0.7952 0.4265  -1.9142  0.8093\n",
       "think            -1.9609   0.7580 -2.5871 0.0097  -3.4466 -0.4753\n",
       "being             0.0170   0.7869  0.0216 0.9828  -1.5253  1.5592\n",
       "said              0.8001   0.7507  1.0658 0.2865  -0.6712  2.2713\n",
       "cnn              10.0868   1.6802  6.0035 0.0000   6.7938 13.3799\n",
       "virus            -0.2292   0.6145 -0.3729 0.7092  -1.4336  0.9752\n",
       "would            -1.5984   0.7673 -2.0831 0.0372  -3.1022 -0.0945\n",
       "your              0.3811   0.7641  0.4987 0.6180  -1.1165  1.8786\n",
       "global           -0.2114   0.7574 -0.2791 0.7802  -1.6960  1.2732\n",
       "no               -1.1677   0.7585 -1.5395 0.1237  -2.6545  0.3190\n",
       "much             -0.4884   0.7343 -0.6652 0.5060  -1.9277  0.9508\n",
       "don't            -2.0235   0.7437 -2.7209 0.0065  -3.4812 -0.5659\n",
       "which            -1.0920   0.6366 -1.7153 0.0863  -2.3398  0.1557\n",
       "thank            -3.1212   0.8971 -3.4793 0.0005  -4.8795 -1.3630\n",
       "deaths            1.9664   0.8814  2.2311 0.0257   0.2389  3.6939\n",
       "say               1.0513   0.6087  1.7270 0.0842  -0.1418  2.2444\n",
       "next              0.4837   0.6629  0.7297 0.4655  -0.8155  1.7830\n",
       "testing           0.3945   0.9172  0.4301 0.6671  -1.4032  2.1922\n",
       "says             -0.5393   0.8021 -0.6724 0.5013  -2.1113  1.0327\n",
       "even              0.3119   0.6781  0.4599 0.6456  -1.0172  1.6410\n",
       "other             0.0128   0.7191  0.0178 0.9858  -1.3966  1.4221\n",
       "two               1.3370   0.7489  1.7853 0.0742  -0.1308  2.8048\n",
       "its              -1.5703   0.7827 -2.0062 0.0448  -3.1044 -0.0362\n",
       "them             -1.5160   0.7768 -1.9516 0.0510  -3.0385  0.0065\n",
       "death            -1.5653   0.8734 -1.7923 0.0731  -3.2770  0.1464\n",
       "many              0.5979   0.8657  0.6906 0.4898  -1.0989  2.2947\n",
       "test              2.0124   1.0148  1.9830 0.0474   0.0233  4.0015\n",
       "they're          -1.3271   0.8088 -1.6407 0.1009  -2.9124  0.2582\n",
       "into             -0.2564   0.7734 -0.3316 0.7402  -1.7724  1.2595\n",
       "these            -1.5811   0.8629 -1.8323 0.0669  -3.2723  0.1101\n",
       "really            1.5791   0.7964  1.9827 0.0474   0.0181  3.1401\n",
       "time             -0.2017   0.8565 -0.2355 0.8138  -1.8804  1.4769\n",
       "see               1.4138   0.7572  1.8671 0.0619  -0.0703  2.8978\n",
       "coming           -1.8303   0.9338 -1.9601 0.0500  -3.6604 -0.0001\n",
       "around           -2.7941   0.9457 -2.9547 0.0031  -4.6476 -0.9407\n",
       "most             -0.4292   0.7981 -0.5377 0.5908  -1.9935  1.1352\n",
       "day               0.4503   0.7510  0.5996 0.5488  -1.0217  1.9224\n",
       "crisis            0.9709   0.7906  1.2281 0.2194  -0.5786  2.5203\n",
       "during            0.8097   0.8435  0.9599 0.3371  -0.8435  2.4630\n",
       "officials         1.0916   0.6926  1.5761 0.1150  -0.2658  2.4491\n",
       "still             0.1479   0.8569  0.1725 0.8630  -1.5316  1.8273\n",
       "take             -0.0272   0.7845 -0.0346 0.9724  -1.5647  1.5104\n",
       "want             -0.6253   0.8417 -0.7428 0.4576  -2.2750  1.0245\n",
       "died              1.3241   0.9362  1.4143 0.1573  -0.5108  3.1589\n",
       "flu              -0.6942   0.8118 -0.8552 0.3925  -2.2853  0.8968\n",
       "i'm              -2.4063   0.7986 -3.0131 0.0026  -3.9716 -0.8411\n",
       "fight             0.7070   0.8937  0.7910 0.4289  -1.0447  2.4586\n",
       "there's           0.9278   0.7124  1.3025 0.1928  -0.4684  2.3240\n",
       "need             -0.9397   0.8207 -1.1451 0.2522  -2.5483  0.6688\n",
       "medical           0.3261   0.9432  0.3458 0.7295  -1.5224  2.1747\n",
       "against          -1.1636   0.9322 -1.2481 0.2120  -2.9907  0.6636\n",
       "any              -1.5043   0.8132 -1.8499 0.0643  -3.0980  0.0895\n",
       "case             -1.1788   0.7650 -1.5409 0.1233  -2.6781  0.3206\n",
       "should           -0.1815   0.7988 -0.2273 0.8202  -1.7472  1.3841\n",
       "chinese          -1.5662   0.9433 -1.6602 0.0969  -3.4151  0.2828\n",
       "briefing         -1.8140   1.0822 -1.6762 0.0937  -3.9352  0.3071\n",
       "last             -0.2482   0.7507 -0.3306 0.7410  -1.7195  1.2231\n",
       "may               2.2439   1.0174  2.2055 0.0274   0.2498  4.2380\n",
       "hospital         -1.4056   0.9674 -1.4530 0.1462  -3.3017  0.4905\n",
       "did               1.3088   0.9016  1.4517 0.1466  -0.4582  3.0758\n",
       "ship              4.9637   1.1551  4.2970 0.0000   2.6996  7.2277\n",
       "me                0.2088   0.7209  0.2896 0.7721  -1.2041  1.6217\n",
       "yes              -0.7332   0.7613 -0.9631 0.3355  -2.2254  0.7590\n",
       "look             -0.1710   0.8315 -0.2056 0.8371  -1.8008  1.4588\n",
       "help              0.4563   0.9264  0.4926 0.6223  -1.3594  2.2721\n",
       "then             -1.4406   1.0218 -1.4099 0.1586  -3.4433  0.5620\n",
       "we'll            -0.2098   0.9204 -0.2279 0.8197  -2.0138  1.5942\n",
       "go                1.4289   0.7709  1.8536 0.0638  -0.0820  2.9398\n",
       "across            2.0072   0.9118  2.2013 0.0277   0.2201  3.7943\n",
       "american         -2.2889   1.0802 -2.1190 0.0341  -4.4060 -0.1718\n",
       "lot               0.3169   0.9579  0.3308 0.7408  -1.5606  2.1944\n",
       "every             0.4041   0.8954  0.4513 0.6517  -1.3508  2.1590\n",
       "another          -0.2425   0.8694 -0.2789 0.7803  -1.9465  1.4615\n",
       "percent          -0.0324   0.9230 -0.0351 0.9720  -1.8415  1.7767\n",
       "down              1.1101   0.8898  1.2476 0.2122  -0.6338  2.8541\n",
       "economy           0.2617   0.7756  0.3374 0.7358  -1.2585  1.7818\n",
       "week              0.8710   0.9695  0.8983 0.3690  -1.0293  2.7712\n",
       "symptoms          1.9041   0.8200  2.3222 0.0202   0.2970  3.5112\n",
       "live              1.0588   0.8917  1.1874 0.2351  -0.6889  2.8065\n",
       "care              2.3414   1.0134  2.3105 0.0209   0.3552  4.3275\n",
       "through           3.8890   1.0662  3.6477 0.0003   1.7994  5.9786\n",
       "way              -3.4729   0.8984 -3.8656 0.0001  -5.2337 -1.7120\n",
       "america          -2.0378   1.1439 -1.7814 0.0748  -4.2798  0.2042\n",
       "make             -0.4007   0.8738 -0.4586 0.6466  -2.1134  1.3120\n",
       "correspondent    -4.3753   1.1735 -3.7284 0.0002  -6.6753 -2.0753\n",
       "home              1.1532   0.8235  1.4004 0.1614  -0.4608  2.7672\n",
       "you're            0.2073   0.8518  0.2434 0.8077  -1.4621  1.8768\n",
       "city              4.4666   1.0847  4.1178 0.0000   2.3407  6.5926\n",
       "before           -2.1507   0.9756 -2.2046 0.0275  -4.0628 -0.2387\n",
       "(end             -2.0658   1.1027 -1.8735 0.0610  -4.2270  0.0953\n",
       "infected          0.6004   0.8727  0.6879 0.4915  -1.1102  2.3109\n",
       "saying           -0.9341   1.0460 -0.8930 0.3718  -2.9842  1.1160\n",
       "birx              9.7789   3.9925  2.4493 0.0143   1.9538 17.6040\n",
       "getting           2.0507   0.8503  2.4117 0.0159   0.3841  3.7172\n",
       "novel             2.4561   1.0152  2.4195 0.0155   0.4665  4.4458\n",
       "days              2.9489   0.9608  3.0691 0.0021   1.0657  4.8321\n",
       "impact            0.4329   0.9159  0.4726 0.6365  -1.3623  2.2281\n",
       "government       -0.8644   0.8755 -0.9874 0.3234  -2.5803  0.8514\n",
       "only             -0.9158   0.7858 -1.1655 0.2438  -2.4559  0.6243\n",
       "he's             -0.2846   0.9139 -0.3114 0.7555  -2.0758  1.5066\n",
       "got              -0.4498   0.8202 -0.5484 0.5834  -2.0573  1.1577\n",
       "emergency         0.6346   0.8467  0.7495 0.4536  -1.0248  2.2940\n",
       "wuhan            -1.1144   0.9045 -1.2320 0.2179  -2.8871  0.6584\n",
       "bill             -5.1435   1.3419 -3.8331 0.0001  -7.7735 -2.5135\n",
       "she               2.4538   1.1491  2.1355 0.0327   0.2017  4.7059\n",
       "washington        1.2737   1.0479  1.2155 0.2242  -0.7801  3.3276\n",
       "fears             2.5457   0.8801  2.8926 0.0038   0.8208  4.2706\n",
       "come             -0.4964   0.9453 -0.5251 0.5995  -2.3491  1.3563\n",
       "economic          0.0328   0.7897  0.0416 0.9668  -1.5149  1.5805\n",
       "my                0.9748   0.9203  1.0592 0.2895  -0.8290  2.7785\n",
       "toll              1.9993   1.2408  1.6113 0.1071  -0.4326  4.4312\n",
       "deborah         -11.3658   4.1021 -2.7707 0.0056 -19.4057 -3.3259\n",
       "why               0.2776   0.8896  0.3121 0.7550  -1.4659  2.0211\n",
       "patient           1.3549   0.8839  1.5329 0.1253  -0.3775  3.0873\n",
       "under             1.3712   0.8634  1.5882 0.1122  -0.3210  3.0635\n",
       "far              -1.5733   0.9724 -1.6180 0.1057  -3.4792  0.3325\n",
       "course            1.3398   0.8542  1.5685 0.1168  -0.3344  3.0141\n",
       "again            -2.7035   0.9334 -2.8963 0.0038  -4.5329 -0.8740\n",
       "spreading        -0.1063   0.9374 -0.1134 0.9097  -1.9436  1.7311\n",
       "least             1.6866   1.1584  1.4560 0.1454  -0.5838  3.9569\n",
       "already           1.2903   0.9347  1.3804 0.1675  -0.5417  3.1224\n",
       "public           -0.8873   0.9796 -0.9058 0.3650  -2.8074  1.0327\n",
       "latest           -0.8115   0.8483 -0.9566 0.3388  -2.4742  0.8512\n",
       "good             -2.0162   0.8973 -2.2470 0.0246  -3.7748 -0.2576\n",
       "actually         -0.4664   1.0521 -0.4433 0.6576  -2.5284  1.5957\n",
       "hospitals         2.9452   1.1303  2.6057 0.0092   0.7299  5.1605\n",
       "weeks             2.4802   0.9355  2.6513 0.0080   0.6467  4.3137\n",
       "comes            -2.6700   1.0550 -2.5307 0.0114  -4.7378 -0.6021\n",
       "administration    0.1036   0.9488  0.1092 0.9130  -1.7560  1.9632\n",
       "italy            -1.3920   0.9896 -1.4066 0.1595  -3.3316  0.5476\n",
       "risk             -1.4968   1.0598 -1.4123 0.1579  -3.5740  0.5804\n",
       "vice             -1.8051   1.0610 -1.7013 0.0889  -3.8846  0.2744\n",
       "point            -0.7907   0.9407 -0.8406 0.4006  -2.6344  1.0529\n",
       "things           -1.7684   0.8735 -2.0246 0.0429  -3.4804 -0.0564\n",
       "disease          -1.0920   0.8845 -1.2347 0.2169  -2.8255  0.6415\n",
       "big              -3.7365   0.8741 -4.2749 0.0000  -5.4496 -2.0233\n",
       "doing            -4.2863   1.0895 -3.9341 0.0001  -6.4217 -2.1509\n",
       "=================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As shown in the results summary above, the model appears to be highly significant (LLR p-value:\t2.8176-79), and there are many words that are significant predictors of a news outlet's identity given all of the other words in the model at both the 0.05 and 0.01 significance levels. The model also has a Pseudo R-squared of 0.725 which is rather surprisingly high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select explanatory variables (words) that distinguish between Fox and CNN news transcripts the most. Pick only those words with a p-value equal to or less than 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table=results_summary.tables[1]\n",
    "most_important_distinguishing_words=results_table[results_table['P>|z|']<0.01].sort_values(['P>|z|'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef.</th>\n",
       "      <th>Std.Err.</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "      <th>[0.025</th>\n",
       "      <th>0.975]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>10.086829</td>\n",
       "      <td>1.680159</td>\n",
       "      <td>6.003497</td>\n",
       "      <td>1.931122e-09</td>\n",
       "      <td>6.793778</td>\n",
       "      <td>13.379880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-4.095652</td>\n",
       "      <td>0.839507</td>\n",
       "      <td>-4.878641</td>\n",
       "      <td>1.068194e-06</td>\n",
       "      <td>-5.741055</td>\n",
       "      <td>-2.450249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tonight</th>\n",
       "      <td>-4.209850</td>\n",
       "      <td>0.955201</td>\n",
       "      <td>-4.407293</td>\n",
       "      <td>1.046707e-05</td>\n",
       "      <td>-6.082010</td>\n",
       "      <td>-2.337691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outbreak</th>\n",
       "      <td>2.630076</td>\n",
       "      <td>0.606607</td>\n",
       "      <td>4.335717</td>\n",
       "      <td>1.452856e-05</td>\n",
       "      <td>1.441148</td>\n",
       "      <td>3.819003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ship</th>\n",
       "      <td>4.963661</td>\n",
       "      <td>1.155144</td>\n",
       "      <td>4.297006</td>\n",
       "      <td>1.731208e-05</td>\n",
       "      <td>2.699620</td>\n",
       "      <td>7.227703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <td>-3.736452</td>\n",
       "      <td>0.874052</td>\n",
       "      <td>-4.274864</td>\n",
       "      <td>1.912543e-05</td>\n",
       "      <td>-5.449563</td>\n",
       "      <td>-2.023342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>4.466628</td>\n",
       "      <td>1.084700</td>\n",
       "      <td>4.117847</td>\n",
       "      <td>3.824277e-05</td>\n",
       "      <td>2.340655</td>\n",
       "      <td>6.592600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doing</th>\n",
       "      <td>-4.286276</td>\n",
       "      <td>1.089510</td>\n",
       "      <td>-3.934133</td>\n",
       "      <td>8.349765e-05</td>\n",
       "      <td>-6.421676</td>\n",
       "      <td>-2.150876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>-3.472856</td>\n",
       "      <td>0.898410</td>\n",
       "      <td>-3.865559</td>\n",
       "      <td>1.108350e-04</td>\n",
       "      <td>-5.233707</td>\n",
       "      <td>-1.712005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>-5.143475</td>\n",
       "      <td>1.341867</td>\n",
       "      <td>-3.833074</td>\n",
       "      <td>1.265519e-04</td>\n",
       "      <td>-7.773485</td>\n",
       "      <td>-2.513464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correspondent</th>\n",
       "      <td>-4.375302</td>\n",
       "      <td>1.173497</td>\n",
       "      <td>-3.728431</td>\n",
       "      <td>1.926754e-04</td>\n",
       "      <td>-6.675314</td>\n",
       "      <td>-2.075291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid19</th>\n",
       "      <td>-2.796459</td>\n",
       "      <td>0.758353</td>\n",
       "      <td>-3.687543</td>\n",
       "      <td>2.264303e-04</td>\n",
       "      <td>-4.282803</td>\n",
       "      <td>-1.310114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>through</th>\n",
       "      <td>3.889018</td>\n",
       "      <td>1.066152</td>\n",
       "      <td>3.647715</td>\n",
       "      <td>2.645828e-04</td>\n",
       "      <td>1.799399</td>\n",
       "      <td>5.978638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>2.319109</td>\n",
       "      <td>0.654821</td>\n",
       "      <td>3.541590</td>\n",
       "      <td>3.977237e-04</td>\n",
       "      <td>1.035682</td>\n",
       "      <td>3.602535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>2.240854</td>\n",
       "      <td>0.640490</td>\n",
       "      <td>3.498658</td>\n",
       "      <td>4.676067e-04</td>\n",
       "      <td>0.985517</td>\n",
       "      <td>3.496191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>-2.616826</td>\n",
       "      <td>0.750317</td>\n",
       "      <td>-3.487629</td>\n",
       "      <td>4.873236e-04</td>\n",
       "      <td>-4.087419</td>\n",
       "      <td>-1.146232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thank</th>\n",
       "      <td>-3.121230</td>\n",
       "      <td>0.897095</td>\n",
       "      <td>-3.479262</td>\n",
       "      <td>5.027959e-04</td>\n",
       "      <td>-4.879505</td>\n",
       "      <td>-1.362956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>-4.921748</td>\n",
       "      <td>1.457777</td>\n",
       "      <td>-3.376202</td>\n",
       "      <td>7.349398e-04</td>\n",
       "      <td>-7.778938</td>\n",
       "      <td>-2.064558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-2.063506</td>\n",
       "      <td>0.664952</td>\n",
       "      <td>-3.103243</td>\n",
       "      <td>1.914123e-03</td>\n",
       "      <td>-3.366787</td>\n",
       "      <td>-0.760225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confirmed</th>\n",
       "      <td>2.414017</td>\n",
       "      <td>0.781743</td>\n",
       "      <td>3.087992</td>\n",
       "      <td>2.015137e-03</td>\n",
       "      <td>0.881829</td>\n",
       "      <td>3.946206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days</th>\n",
       "      <td>2.948919</td>\n",
       "      <td>0.960841</td>\n",
       "      <td>3.069102</td>\n",
       "      <td>2.147033e-03</td>\n",
       "      <td>1.065705</td>\n",
       "      <td>4.832132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i'm</th>\n",
       "      <td>-2.406314</td>\n",
       "      <td>0.798616</td>\n",
       "      <td>-3.013107</td>\n",
       "      <td>2.585877e-03</td>\n",
       "      <td>-3.971572</td>\n",
       "      <td>-0.841056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>2.463981</td>\n",
       "      <td>0.822256</td>\n",
       "      <td>2.996610</td>\n",
       "      <td>2.730001e-03</td>\n",
       "      <td>0.852388</td>\n",
       "      <td>4.075573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>-2.015411</td>\n",
       "      <td>0.680098</td>\n",
       "      <td>-2.963413</td>\n",
       "      <td>3.042477e-03</td>\n",
       "      <td>-3.348378</td>\n",
       "      <td>-0.682444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>around</th>\n",
       "      <td>-2.794130</td>\n",
       "      <td>0.945662</td>\n",
       "      <td>-2.954682</td>\n",
       "      <td>3.129917e-03</td>\n",
       "      <td>-4.647594</td>\n",
       "      <td>-0.940667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Coef.  Std.Err.         z         P>|z|    [0.025  \\\n",
       "cnn            10.086829  1.680159  6.003497  1.931122e-09  6.793778   \n",
       "a              -4.095652  0.839507 -4.878641  1.068194e-06 -5.741055   \n",
       "tonight        -4.209850  0.955201 -4.407293  1.046707e-05 -6.082010   \n",
       "outbreak        2.630076  0.606607  4.335717  1.452856e-05  1.441148   \n",
       "ship            4.963661  1.155144  4.297006  1.731208e-05  2.699620   \n",
       "big            -3.736452  0.874052 -4.274864  1.912543e-05 -5.449563   \n",
       "city            4.466628  1.084700  4.117847  3.824277e-05  2.340655   \n",
       "doing          -4.286276  1.089510 -3.934133  8.349765e-05 -6.421676   \n",
       "way            -3.472856  0.898410 -3.865559  1.108350e-04 -5.233707   \n",
       "bill           -5.143475  1.341867 -3.833074  1.265519e-04 -7.773485   \n",
       "correspondent  -4.375302  1.173497 -3.728431  1.926754e-04 -6.675314   \n",
       "covid19        -2.796459  0.758353 -3.687543  2.264303e-04 -4.282803   \n",
       "through         3.889018  1.066152  3.647715  2.645828e-04  1.799399   \n",
       "so              2.319109  0.654821  3.541590  3.977237e-04  1.035682   \n",
       "just            2.240854  0.640490  3.498658  4.676067e-04  0.985517   \n",
       "news           -2.616826  0.750317 -3.487629  4.873236e-04 -4.087419   \n",
       "thank          -3.121230  0.897095 -3.479262  5.027959e-04 -4.879505   \n",
       "coronavirus    -4.921748  1.457777 -3.376202  7.349398e-04 -7.778938   \n",
       "to             -2.063506  0.664952 -3.103243  1.914123e-03 -3.366787   \n",
       "confirmed       2.414017  0.781743  3.087992  2.015137e-03  0.881829   \n",
       "days            2.948919  0.960841  3.069102  2.147033e-03  1.065705   \n",
       "i'm            -2.406314  0.798616 -3.013107  2.585877e-03 -3.971572   \n",
       "like            2.463981  0.822256  2.996610  2.730001e-03  0.852388   \n",
       "but            -2.015411  0.680098 -2.963413  3.042477e-03 -3.348378   \n",
       "around         -2.794130  0.945662 -2.954682  3.129917e-03 -4.647594   \n",
       "\n",
       "                  0.975]  \n",
       "cnn            13.379880  \n",
       "a              -2.450249  \n",
       "tonight        -2.337691  \n",
       "outbreak        3.819003  \n",
       "ship            7.227703  \n",
       "big            -2.023342  \n",
       "city            6.592600  \n",
       "doing          -2.150876  \n",
       "way            -1.712005  \n",
       "bill           -2.513464  \n",
       "correspondent  -2.075291  \n",
       "covid19        -1.310114  \n",
       "through         5.978638  \n",
       "so              3.602535  \n",
       "just            3.496191  \n",
       "news           -1.146232  \n",
       "thank          -1.362956  \n",
       "coronavirus    -2.064558  \n",
       "to             -0.760225  \n",
       "confirmed       3.946206  \n",
       "days            4.832132  \n",
       "i'm            -0.841056  \n",
       "like            4.075573  \n",
       "but            -0.682444  \n",
       "around         -0.940667  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_important_distinguishing_words[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 36 words that distinguish between CNN and Fox news transcripts at the 99% significance level. They are found in the table above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most important words (categorized at the 0.01 significance level) found between Fox and CNN news broadcasts about the coronavirus that distinguish between the two can be found above. The most meaningful of these words, excluding filler words, include 'outbreak' , 'covid19', 'symptoms', 'president', and 'hospitals' to name a few. These words are the most important and statistically significant words/predictors that are able to distinguish between Fox and CNN news coverage of the coronavirus. The words above will be used to create a more robust logistic regression model using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn to Predict News Outlet Identity \n",
    "### Creating Second Logistic Regression Model Fitting on training data-- Predictors Narrowed Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use only most important words found in the logistic regression above:\n",
    "cols_x_forPred=most_important_distinguishing_words.index\n",
    "col_y_forPred=dtm.columns[1000]\n",
    "X_forPred=dtm[cols_x_forPred]\n",
    "y_forPred=dtm[col_y_forPred]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_forPred, y_forPred, test_size=0.3, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the test set results using the above model and calculating the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.75\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.75\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.73       134\n",
      "           1       0.75      0.77      0.76       144\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       278\n",
      "   macro avg       0.75      0.75      0.75       278\n",
      "weighted avg       0.75      0.75      0.75       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above confusion matrix and accuracy calculation show the results of the model I created after fitting it to the training data, testing it it on the test data, and then comparing the results of my predictions to the actual values in the test data.\n",
    "\n",
    "### Overall, it appears that the model I created does a rather good job at predicting news outlet identity using the important words that I identified which distinguish between Fox and CNN news broadcasts. The model is accurate at predicting news outlet identity 80% of the time, which is much higher than what would occur via random guessing. The quality of my logistic regression is further bolstered by the output of the above classification report which shows high precision and recall scores, two additional metrics which show classification model performance and quality, by comparing true positive rates to either the total number of positive classifications the model makes (precision) or the total number of positives in the data regardless of the classifications made by the model (recall).\n",
    "\n",
    "### Therefore, this model and logistic regression exercise help to reveal that:\n",
    "\n",
    "###         1. Vocabulary differences between Fox and CNN news broadcasts are real, meaningful, and important, and                      represent key differences between each of the news outlet's coronavirus responses; CNN and Fox                              significantly differ in their word choice, leading to differing responses to coronavirus.\n",
    "\n",
    "###         2. These vocabulary differences can be used to distinguish between Fox and CNN broadcasts in a robust                        fashion, creating a highly accurate classification model.\n",
    "\n",
    "###         3. Certain words are more important than others when it comes to distinguishing between the news                                outlet's news broadcasts and coronavirus responses.\n",
    "\n",
    "###         4. Logistic Regression helped to confirm the results of Keyness Analysis by revealing many of the same                          words which Fox or CNN use significantly more relative to one another. For example, Keyness Analysis                      showed that CNN uses the words 'symptoms' and 'outbreak' significantly more than Fox does. Keyness                      Analysis also showed that Fox uses the words 'democrats', 'covid19', 'china', and 'chinese' significantly                      more than CNN does. This was all confirmed by the logistic regression I created, which confirms that                          these words are very important at distinguishing between the news outlets' resposnes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
