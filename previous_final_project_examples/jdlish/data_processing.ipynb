{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook was used to preliminarily process and clean the csv data files I created from raw Lexis Nexis xml files. Some additional cleaning may occur while doing data analysis, as additional issue or problems with the raw data arise.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Commjhub/jupyterhub/comm318_fall2019/jdlish/nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Commjhub/jupyterhub/comm318_fall2019/jdlish/nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "%run Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Data Preliminary Processing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a CNN dataframe\n",
    "data_cnn = pd.read_csv('Data Files/CNN and Fox Csv Files/cnn_data.csv', sep='|', header=None,error_bad_lines=False, warn_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnn.columns = [\"full_date\",\"body_text\",\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnn[\"News Outlet\"]=\"CNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column tokenizing the text of each news transcript. This will save time later in each analysis step.\n",
    "strip_chars=';:.\"--,!$%^&*@#'\n",
    "data_cnn[\"Tokenized Text\"]=data_cnn[\"body_text\"].apply(tokenize,args=(True,strip_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dates into strings to remove problems with floats vs. strings in the same column\n",
    "data_cnn['full_date']=data_cnn['full_date'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fox Data Preliminary Processing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Fox dataframe\n",
    "data_fox = pd.read_csv('Data Files/CNN and Fox Csv Files/fox_data.csv', sep='|', header=None,error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fox.columns = [\"full_date\",\"body_text\",\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fox[\"News Outlet\"]=\"Fox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column tokenizing the text of each news transcript. This will save time later in each analysis step.\n",
    "strip_chars=';:.\"--,!$%^&*@#'\n",
    "data_fox[\"Tokenized Text\"]=data_fox[\"body_text\"].apply(tokenize,args=(True,strip_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dates into strings to remove problems with floats vs. strings in the same column\n",
    "data_fox['full_date']=data_fox['full_date'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each news transcript from both Fox and CNN that appears in the above dataframe contains coverage about the coronavirus. However, each news transcript tends to be very long (thousands of words each). From reading a few news transcripts, it appears that the news transcripts contain information not solely pertaining to the coronavirus directly, including tangents, side conversations between news anchors, and numerous filler words/topics not directly related to coronavirus directly. Therefore, for my data analysis I decided to only focus attention on the portions of each news transcript that directly pertain to coronavirus. I did this by targeting the key words 'coronavirus', 'pandemic', or 'covid19'. I created what I call \"targeted text\", which I feel gives me a more accurate representation of each news outlet's coronavirus response. I believe that focusing on this \"targeted text\" will eliminate a lot of noise from the news transcripts. To create these targeted texts, I wrote a function that creates KWICS for each news transcript based on the above three key words, and then flattens the KWICS to create a single and cohesive corpus for each news transcript. I included 10 words before and after each key word in each flattened quick. Doing this would enable me to create an accurate representation of each news outlet's coronavirus response, by focusing directly on the area of each news transcript that mentions coronavirus. This function is called make_flattened_kwic and can be found in my functions notebook.\n",
    "\n",
    "### The code below creates a new feature in my CNN and Fox dataframes called 'targeted text,' which contains the information as outlined above. I will conduct all of my data analysis using targeted text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=['coronavirus',\"pandemic\",\"covid19\"]\n",
    "#For fox\n",
    "\n",
    "new_col_fox2=[]\n",
    "for i in range(len(data_fox['body_text'])):\n",
    "    text=data_fox['Tokenized Text'][i]\n",
    "    flattened_kwic=make_flattened_kwic(kw=keys,text=text,win=10)\n",
    "    new_col_fox2.append(flattened_kwic)\n",
    "\n",
    "#For CNN\n",
    "\n",
    "new_col_cnn2=[]\n",
    "keys=['coronavirus',\"pandemic\",\"covid19\"]\n",
    "for i in range(len(data_cnn['body_text'])):\n",
    "    text=data_cnn['Tokenized Text'][i]\n",
    "    flattened_kwic=make_flattened_kwic(kw=keys,text=text,win=10)\n",
    "    new_col_cnn2.append(flattened_kwic)\n",
    "\n",
    "#Add 'targeted text' as a new column to each dataframe\n",
    "\n",
    "data_fox['targeted text']=new_col_fox2\n",
    "data_cnn['targeted text']=new_col_cnn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexis nexus was supposed to only get news broadcasts that had the words 'coronavirus','pandemic', or 'covid19'. For some reason some of the broadcasts don't contain any of those words so I am removing them from both the CNN and Fox data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "data_cnn=data_cnn[data_cnn['targeted text']!=''].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fox\n",
    "data_fox=data_fox[data_fox['targeted text']!=''].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
