{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# In this Notebook I aggregated all my functions\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyness(fdist1, fdist2, fthreshold=5, keyness_threshold=6.6, top=100, print_table=True):\n",
    "    '''create a keyness comparison table from two frequency lists\n",
    "    '''\n",
    "    \n",
    "    c1size = sum(fdist1.values())\n",
    "    c2size = sum(fdist2.values())\n",
    "\n",
    "    \n",
    "    kdata = []\n",
    "    \n",
    "    for item, freq in fdist1.items():\n",
    "        if freq<fthreshold:\n",
    "            continue\n",
    "            \n",
    "        ref_freq = fdist2.get(item,0)\n",
    "        \n",
    "        if ref_freq<fthreshold:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        keyness = log_likelihood(freq, c1size, ref_freq, c2size)\n",
    "        \n",
    "        row = {'item': item, 'freq': freq, 'ref_freq': ref_freq, 'keyness': keyness}\n",
    "        \n",
    "        if keyness>keyness_threshold:\n",
    "        \n",
    "            kdata.append(row)\n",
    "        \n",
    "    \n",
    "    kdf = pd.DataFrame(kdata)[['item', 'freq', 'ref_freq', 'keyness']]\n",
    "    \n",
    "    kdf=kdf.sort_values('keyness', ascending=False)\n",
    "    \n",
    "    if not print_table:\n",
    "        return kdf[:top]\n",
    "    \n",
    "    template = \"{: <25}{: <10}{: <10}{:0.3f}\"\n",
    "    \n",
    "    header = \"{: <25}{: <10}{: <10}{}\".format('WORD', 'Corpus A Freq.', 'Corpus B Freq.', 'Keyness')\n",
    "    \n",
    "    print(\"{}\\n{}\".format(header, \"=\"*len(header)))\n",
    "    \n",
    "    for item, freq, ref_freq, keyness in kdf[:top].values:\n",
    "        print(template.format(item, freq, ref_freq, keyness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(item_A_freq, corpus_A_size, item_B_freq, corpus_B_size):\n",
    "    '''calculate the log likelihood score for a comparison between the frequency of two items\n",
    "    '''\n",
    "    E1 = corpus_A_size*(item_A_freq+item_B_freq) / (corpus_A_size+corpus_B_size)\n",
    "    E2 = corpus_B_size*(item_A_freq+item_B_freq) / (corpus_A_size+corpus_B_size)\n",
    "\n",
    "    G2 = 2*((item_A_freq*math.log(item_A_freq/E1)) + (item_B_freq*math.log(item_B_freq/E2)))\n",
    "    \n",
    "    sign = 1 if (item_A_freq / corpus_A_size) >= (item_B_freq / corpus_B_size) else -1\n",
    "    \n",
    "    return sign*G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lowercase=False, strip_chars=''):\n",
    "    '''create a list of tokens from a string by splitting on whitespace and applying optional normalization \n",
    "    \n",
    "    Args:\n",
    "        text        -- a string object containing the text to be tokenized\n",
    "        lowercase   -- should text string be normalized as lowercase (default: False)\n",
    "        strip_chars -- a string indicating characters to strip out of text, e.g. punctuation (default: empty string) \n",
    "        \n",
    "    Return:\n",
    "        A list of tokens\n",
    "    '''\n",
    "    \n",
    "    # create a replacement dictionary from the\n",
    "    # string of characters in the **strip_chars**\n",
    "    rdict = str.maketrans('','',strip_chars)\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    tokens = text.translate(rdict).split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_tokens(tokens, n=1):\n",
    "    '''create a list of n-gram tokens from a list of tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens -- a list of tokens\n",
    "        n      -- the size of the window to use to build n-gram token list\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "        list of n-gram strings (whitespace separated) of length n\n",
    "    '''\n",
    "    \n",
    "    if n<2 or n>len(tokens):\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    for i in range(len(tokens)-n+1):\n",
    "        new_tokens.append(\" \".join(tokens[i:i+n]))\n",
    "        \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kwic(kw, text, win=4):\n",
    "    '''A basic KWIC function for a text\n",
    "    \n",
    "    Args:\n",
    "        kw   -- string match for keyword to match for each line\n",
    "        text -- a list of tokens for the text\n",
    "        \n",
    "    Return:\n",
    "        list of lines of form [ [left context words], kw, [right context words]]\n",
    "    '''\n",
    "    \n",
    "    hits = [(w,i) for i,w in enumerate(text) if w==kw]\n",
    "    \n",
    "    lines = []\n",
    "    for hit in hits:\n",
    "        left = text[hit[1]-win:hit[1]]\n",
    "        kw = text[hit[1]]\n",
    "        right = text[hit[1]+1 : hit[1]+win+1]\n",
    "        \n",
    "        \n",
    "        left = ['']*(win-len(left)) + left if len(left)<win else left\n",
    "        right = right+['']*(win-len(right)) if len(right)<win else right\n",
    "\n",
    "        \n",
    "        lines.append([left, kw, right])\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kwic(kwic, win=None):\n",
    "    '''A basic print function for a KWIC object\n",
    "    \n",
    "    Args:\n",
    "        kwic -- a list of KWIC lines of the form [ [left words], kw, [right words]]\n",
    "        win  -- if None then use all words provided in context otherwise limit by win\n",
    "        \n",
    "    Prints KWIC lines with left context width/padding win*8 characters\n",
    "    '''\n",
    "    \n",
    "    if not kwic:\n",
    "        return\n",
    "    \n",
    "    if win is None:\n",
    "        win = len(kwic[0][0])\n",
    "    \n",
    "    for line in kwic:\n",
    "        print(\"{: >{}}  {}  {}\".format(' '.join(line[0][-win:]), \n",
    "                                      win*10, \n",
    "                                      line[1], \n",
    "                                      ' '.join(line[2][:win])\n",
    "                                     )\n",
    "             )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_kwic(kwic, order=None):\n",
    "    ''' sort a kwic list using the passed positional arguments \n",
    "    \n",
    "    Args:\n",
    "        kwic   -- a list of lists [ [left tokens], kw, [right tokens]]\n",
    "        order  -- a list of one or more positional arguments of form side-pos, e.g. L1, R3, L4 (default: None)\n",
    "    \n",
    "    Returns:\n",
    "        kwic sorted for each positional argument in reverse, i.e. ['R1','L1'] sorts first by L1 and then R1\n",
    "    '''\n",
    "    if order is None:\n",
    "        return kwic\n",
    "   \n",
    "    order = [order] if not type(order) is list else order\n",
    "    order.reverse()\n",
    "    \n",
    "    for sort_term in order:\n",
    "        if not re.match('[LR][1-4]', sort_term):\n",
    "            pass\n",
    "        \n",
    "        pos1 = 0 if sort_term[0]=='L' else 2\n",
    "        pos2 = int(sort_term[1])-1\n",
    "        pos2 = 3-pos2 if sort_term[0]=='L' else pos2\n",
    "        kwic.sort(key=lambda l : l[pos1][pos2])\n",
    "    \n",
    "    return kwic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocates(tokens, kw, win=[4,4]):\n",
    "    '''return the collocates in a window around a given keyword\n",
    "    \n",
    "    Args:\n",
    "          tokens -- a list of tokens\n",
    "          kw     -- keyword string to find and get collocates for\n",
    "          win    -- a list of number of tokens to left (index 0) and right (index 1) to use; default: [4,4]\n",
    "    \n",
    "    Returns:\n",
    "          a list of contexts (matching window specification) around each instance of keyword in tokens\n",
    "    '''\n",
    "    hits = [p for p,t in enumerate(tokens) if t==kw]\n",
    "    \n",
    "    context=[]\n",
    "    for hit in hits:\n",
    "        left = [] if win[0]<1 else tokens[hit-win[0]:hit]\n",
    "        right = [] if win[1]<1 else tokens[hit+1:hit+win[1]+1]\n",
    "        \n",
    "        context.extend(left)\n",
    "        context.extend(right)\n",
    "        \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colls(texts,kw, win=[4,4]):\n",
    "    '''create a collocate frequency list for instances of a kw in a list of texts\n",
    "    \n",
    "    Args:\n",
    "        texts  -- a list of tokenized texts\n",
    "        kw     -- keyword string to find and get collocates for\n",
    "        win    -- a list of number of tokens to left (index 0) and right (index 1) to use; default: [4,4]\n",
    "    \n",
    "    Returns:\n",
    "        a list-of-tuples where each tuple is (collocate, freq_with_kw, coll_total_freq)\n",
    "    '''\n",
    "    word_dist = Counter()\n",
    "    colls = Counter()\n",
    "    for text, tokens in texts.items():\n",
    "        word_dist.update(tokens)\n",
    "        colls.update(collocates(tokens,kw, win))\n",
    "    \n",
    "    return [(str(k),v, word_dist[k]) for k,v in colls.items()], word_dist.get(kw), sum(word_dist.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(tfile):\n",
    "    \n",
    "    tweets = []\n",
    "    for line in open(tfile):\n",
    "        try:\n",
    "            tweets.append(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    try:\n",
    "        toks = tt.tokenize(tweet['text'])\n",
    "    \n",
    "        tweet['tokens'] = toks\n",
    "    \n",
    "        tweet['VAD_toks'] = [] \n",
    "        tweet['Valence']=0\n",
    "        tweet['Dominance']=0\n",
    "        tweet['Arousal']=0\n",
    "    \n",
    "        for t in toks:\n",
    "            if t.lower() in NRC_VAD.keys():\n",
    "                scores = NRC_VAD[t.lower()]\n",
    "                scores['tok']=t\n",
    "            \n",
    "                tweet['Valence']+=scores['V']\n",
    "                tweet['Arousal']+=scores['A']\n",
    "                tweet['Dominance']+=scores['D']\n",
    "            \n",
    "                tweet['VAD_toks'].append(scores)\n",
    "    \n",
    "    \n",
    "        for dimension in ('Valence','Arousal','Dominance'):\n",
    "            if len(tweet['VAD_toks'])>0:\n",
    "                tweet[dimension] /= len(tweet['VAD_toks'])\n",
    "    except:\n",
    "        toks = tt.tokenize(tweet['content'])\n",
    "        tweet['tokens'] = toks\n",
    "    \n",
    "        tweet['VAD_toks'] = [] \n",
    "        tweet['Valence']=0\n",
    "        tweet['Dominance']=0\n",
    "        tweet['Arousal']=0\n",
    "    \n",
    "        for t in toks:\n",
    "            if t.lower() in NRC_VAD.keys():\n",
    "                scores = NRC_VAD[t.lower()]\n",
    "                scores['tok']=t\n",
    "            \n",
    "                tweet['Valence']+=scores['V']\n",
    "                tweet['Arousal']+=scores['A']\n",
    "                tweet['Dominance']+=scores['D']\n",
    "            \n",
    "                tweet['VAD_toks'].append(scores)\n",
    "    \n",
    "    \n",
    "        for dimension in ('Valence','Arousal','Dominance'):\n",
    "            if len(tweet['VAD_toks'])>0:\n",
    "                tweet[dimension] /= len(tweet['VAD_toks'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(A, B, AB, N):\n",
    "    '''calculate pointwise mutual information for a pair of words given their co-occurring frequency and total frequencies\n",
    "    \n",
    "    Args:\n",
    "        A   -- total frequency of word 1\n",
    "        B   -- total frequency of word 1\n",
    "        AB  -- frequency of word 1 and word 2 together\n",
    "        N   -- number of tokens in corpus/sample\n",
    "        \n",
    "    Returns:\n",
    "        the PMI value   log2( AB / A*B * N)\n",
    "    '''\n",
    "    return math.log2(N* (AB / (A * B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(lst):\n",
    "    return round(sum(lst) / len(lst),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_range(df, field, range_from, range_to, range_step=1, fill_with=0):\n",
    "    return df\\\n",
    "      .merge(how='right', on=field,\n",
    "            right = pd.DataFrame({field:np.arange(range_from, range_to, range_step)}))\\\n",
    "      .sort_values(by=field).reset_index().fillna(fill_with).drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
