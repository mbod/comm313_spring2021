{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "#### Getting tweets for the 2020 Democratic Primary about male vs. female candidates\n",
    "To get tweets from the month between the Iowa primary and when Elizabeth Warren dropped out of the race I used the GetOldTweets module. I got tweets by each week of the month and they are organized by candidates. I chose four candidates, the two male and two female candidates that were most prominent during the time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and install everything necessary and get all functions for the data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GetOldTweets3 in /Users/ochroch/.local/lib/python3.7/site-packages (0.0.11)\r\n",
      "Requirement already satisfied: pyquery>=1.2.10 in /Users/ochroch/.local/lib/python3.7/site-packages (from GetOldTweets3) (1.4.1)\r\n",
      "Requirement already satisfied: lxml>=3.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from GetOldTweets3) (4.5.0)\r\n",
      "Requirement already satisfied: cssselect>0.7.9 in /Users/ochroch/.local/lib/python3.7/site-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --user GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import GetOldTweets3 as got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_query_tweets(query, date_since, date_until, max=1000):\n",
    "    print(f\"Downloading tweets for query: '{query}' from {date_since} to {date_until}\")\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query)\\\n",
    "                                               .setSince(date_since)\\\n",
    "                                               .setUntil(date_until)\\\n",
    "                                               .setMaxTweets(max)\n",
    "\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    list_of_tweets = [tweet.__dict__ for tweet in tweets]\n",
    "    return list_of_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonconverter(o):\n",
    "    if isinstance(o, datetime.datetime):\n",
    "        return o.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather data about candidates\n",
    "Get the data using queries which are the twitter username of the candidate and the name that the candidate is know by, which is a part of the candidates name. Put the data in a folder that is the last name of the candidate and the week that the tweets are from. Tweets are again being gathered using GetOldTweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@ewarren' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'warren' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Warren' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/WarrenWk1'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-03\"\n",
    "until = \"2020-02-09\"\n",
    "queries = [\"@ewarren\", 'warren', 'Warren']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@amyklobuchar' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'klobuchar' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Klobuchar' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/KlobucharWk1'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-03\"\n",
    "until = \"2020-02-09\"\n",
    "queries = ['@amyklobuchar','klobuchar', 'Klobuchar']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@BernieSanders' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'bernie' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Bernie' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/SandersWk1'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-03\"\n",
    "until = \"2020-02-09\"\n",
    "queries = ['@BernieSanders', 'bernie', 'Bernie']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@JoeBiden' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Biden' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'biden' from 2020-02-03 to 2020-02-09\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/BidenWk1'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-03\"\n",
    "until = \"2020-02-09\"\n",
    "queries = ['@JoeBiden', 'Biden', 'biden']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@ewarren' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'warren' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Warren' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/WarrenWk2'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-10\"\n",
    "until = \"2020-02-16\"\n",
    "queries = [\"@ewarren\", 'warren', 'Warren']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@amyklobuchar' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'klobuchar' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Klobuchar' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/KlobucharWk2'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-10\"\n",
    "until = \"2020-02-16\"\n",
    "queries = ['@amyklobuchar','klobuchar', 'Klobuchar']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@BernieSanders' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'bernie' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Bernie' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/SandersWk2'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-10\"\n",
    "until = \"2020-02-16\"\n",
    "queries = ['@BernieSanders', 'bernie', 'Bernie']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@JoeBiden' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Biden' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'biden' from 2020-02-10 to 2020-02-16\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/BidenWk2'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-10\"\n",
    "until = \"2020-02-16\"\n",
    "queries = ['@JoeBiden', 'Biden', 'biden']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@ewarren' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'warren' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Warren' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/WarrenWk3'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-17\"\n",
    "until = \"2020-02-23\"\n",
    "queries = [\"@ewarren\", 'warren', 'Warren']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@amyklobuchar' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'klobuchar' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Klobuchar' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/KlobucharWk3'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-17\"\n",
    "until = \"2020-02-23\"\n",
    "queries = ['@amyklobuchar','klobuchar', 'Klobuchar']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@BernieSanders' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'bernie' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Bernie' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/SandersWk3'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-17\"\n",
    "until = \"2020-02-23\"\n",
    "queries = ['@BernieSanders', 'bernie', 'Bernie']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@JoeBiden' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Biden' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'biden' from 2020-02-17 to 2020-02-23\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/BidenWk3'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-17\"\n",
    "until = \"2020-02-23\"\n",
    "queries = ['@JoeBiden', 'Biden', 'biden']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@ewarren' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'warren' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Warren' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/WarrenWk4'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-24\"\n",
    "until = \"2020-03-01\"\n",
    "queries = [\"@ewarren\", 'warren', 'Warren']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@amyklobuchar' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'klobuchar' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Klobuchar' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/KlobucharWk4'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-24\"\n",
    "until = \"2020-03-01\"\n",
    "queries = ['@amyklobuchar','klobuchar', 'Klobuchar']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@BernieSanders' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'bernie' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Bernie' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/SandersWk4'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-24\"\n",
    "until = \"2020-03-01\"\n",
    "queries = ['@BernieSanders', 'bernie', 'Bernie']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@JoeBiden' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Biden' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'biden' from 2020-02-24 to 2020-03-01\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/BidenWk4'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-02-24\"\n",
    "until = \"2020-03-01\"\n",
    "queries = ['@JoeBiden', 'Biden', 'biden']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@ewarren' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'warren' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Warren' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/WarrenWk5'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-03-02\"\n",
    "until = \"2020-03-05\"\n",
    "queries = [\"@ewarren\", 'warren', 'Warren']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@amyklobuchar' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'klobuchar' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Klobuchar' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/KlobucharWk5'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-03-02\"\n",
    "until = \"2020-03-05\"\n",
    "queries = ['@amyklobuchar','klobuchar', 'Klobuchar']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@BernieSanders' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'bernie' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Bernie' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/SandersWk5'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-03-02\"\n",
    "until = \"2020-03-05\"\n",
    "queries = ['@BernieSanders', 'bernie', 'Bernie']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: '@JoeBiden' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'Biden' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n",
      "Downloading tweets for query: 'biden' from 2020-03-02 to 2020-03-05\n",
      "Downloaded 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/BidenWk5'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-03-02\"\n",
    "until = \"2020-03-05\"\n",
    "queries = ['@JoeBiden', 'Biden', 'biden']\n",
    "\n",
    "for query in queries:\n",
    "    tweet_list = download_query_tweets(query, since, until)\n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('Downloaded {} tweets...\\n'.format(len(tweet_list)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        for tweet in tweet_list:\n",
    "            out.write(json.dumps(tweet, default=jsonconverter) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
