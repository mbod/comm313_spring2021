{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data from Data_Gathering notebook\n",
    "Data was collected by week and candidate (Biden, Klobuchar, Sanders, Warren) and includes all kinds of tweets. I wanted to clean so that each candidate has one corpus of tweets and to exclude retweets to get only original tweets, so analysis is easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules, name the sentiment intensity analyzer, name list of stopwords, and name tweet tokernizer \"tt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer, WordPunctTokenizer\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist=stopwords.raw('english').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name new working directory and download functions from functions folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how data is organized in folders and the number of documents in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BidenWk1\t2 texts\n",
      "SandersWk1\t2 texts\n",
      "WarrenWk1\t2 texts\n",
      "KlobucharWk1\t2 texts\n",
      "BidenWk2\t2 texts\n",
      "BidenWk3\t2 texts\n",
      "BidenWk4\t2 texts\n",
      "BidenWk5\t2 texts\n",
      "KlobucharWk2\t2 texts\n",
      "KlobucharWk3\t2 texts\n",
      "KlobucharWk4\t2 texts\n",
      "KlobucharWk5\t2 texts\n",
      "SandersWk2\t2 texts\n",
      "SandersWk3\t2 texts\n",
      "SandersWk4\t2 texts\n",
      "SandersWk5\t2 texts\n",
      "WarrenWk2\t2 texts\n",
      "WarrenWk4\t2 texts\n",
      "WarrenWk5\t2 texts\n",
      "WarrenWk3\t2 texts\n"
     ]
    }
   ],
   "source": [
    "for item in os.listdir('data'):\n",
    "    if item.startswith('.'):\n",
    "        continue\n",
    "    num_of_docs = len([f for f in os.listdir(os.path.join('data', item)) \n",
    "                       if f.endswith('.json')])\n",
    "    print('{}\\t{} texts'.format(item, num_of_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the tweets and starting analysis\n",
    "The purpose of cleaning the data in this way is to get all the tweets in one corpus for analysis and to get only original tweets about candidates\n",
    "\n",
    "#### For all of the tweets about Biden:\n",
    "* Load all of the tweets from each week and name\n",
    "* Make a compound list of dictionaries of all the tweets using a for loop and the extend function\n",
    "* Make a list of dictionaries of the tweets that does not include retweets using loop, if statements and append function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Biden1 = load_tweets('data/BidenWk1/@JoeBiden_2020-02-03_to_2020-02-09.json')\n",
    "\n",
    "Biden2 = load_tweets('data/BidenWk1/Biden_2020-02-03_to_2020-02-09.json')\n",
    "\n",
    "Biden3 = load_tweets('data/BidenWk2/@JoeBiden_2020-02-10_to_2020-02-16.json')\n",
    "\n",
    "Biden4 = load_tweets('data/BidenWk2/Biden_2020-02-10_to_2020-02-16.json')\n",
    "\n",
    "Biden5 = load_tweets('data/BidenWk3/@JoeBiden_2020-02-17_to_2020-02-23.json')\n",
    "\n",
    "Biden6 = load_tweets('data/BidenWk3/Biden_2020-02-17_to_2020-02-23.json')\n",
    "\n",
    "Biden7 = load_tweets('data/BidenWk4/@JoeBiden_2020-02-24_to_2020-03-01.json')\n",
    "\n",
    "Biden8 = load_tweets('data/BidenWk4/Biden_2020-02-24_to_2020-03-01.json')\n",
    "\n",
    "Biden9 = load_tweets('data/BidenWk5/@JoeBiden_2020-03-02_to_2020-03-05.json')\n",
    "\n",
    "Biden10 = load_tweets('data/BidenWk5/Biden_2020-03-02_to_2020-03-05.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Biden_tweets = []\n",
    "for tweets in (Biden1, Biden2, Biden3, Biden4, Biden5, Biden6, Biden7, Biden8, Biden9, Biden10):\n",
    "    Biden_tweets.extend(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Biden_tweets_nrt = []\n",
    "for tweet in Biden_tweets:\n",
    "    if 'RT @' not in tweet['text']:\n",
    "        Biden_tweets_nrt.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For all of the tweets about Klobuchar:\n",
    "* Load all of the tweets from each week and name\n",
    "* Make a compound list of dictionaries of all the tweets using a for loop and the extend function\n",
    "* Make a list of dictionaries of the tweets that does not include retweets using loop, if statements and append function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Klobuchar1 = load_tweets('data/KlobucharWk1/@amyklobuchar_2020-02-03_to_2020-02-09 (1).json')\n",
    "\n",
    "Klobuchar2 = load_tweets('data/KlobucharWk1/klobuchar_2020-02-03_to_2020-02-09 (1).json')\n",
    "\n",
    "Klobuchar3 = load_tweets('data/KlobucharWk2/@amyklobuchar_2020-02-10_to_2020-02-16.json')\n",
    "\n",
    "Klobuchar4 = load_tweets('data/KlobucharWk2/klobuchar_2020-02-10_to_2020-02-16.json')\n",
    "\n",
    "Klobuchar5 = load_tweets('data/KlobucharWk3/@amyklobuchar_2020-02-17_to_2020-02-23.json')\n",
    "\n",
    "Klobuchar6 = load_tweets('data/KlobucharWk3/klobuchar_2020-02-17_to_2020-02-23.json')\n",
    "\n",
    "Klobuchar7 = load_tweets('data/KlobucharWk4/@amyklobuchar_2020-02-24_to_2020-03-01.json')\n",
    "\n",
    "Klobuchar8 = load_tweets('data/KlobucharWk4/klobuchar_2020-02-24_to_2020-03-01.json')\n",
    "\n",
    "Klobuchar9 = load_tweets('data/KlobucharWk5/@amyklobuchar_2020-03-02_to_2020-03-05.json')\n",
    "\n",
    "Klobuchar10 = load_tweets('data/KlobucharWk5/klobuchar_2020-03-02_to_2020-03-05.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Klob_tweets = []\n",
    "for tweets in (Klobuchar1, Klobuchar2, Klobuchar3, Klobuchar4, Klobuchar5, Klobuchar6, Klobuchar7, Klobuchar8, Klobuchar9, Klobuchar10):\n",
    "    Klob_tweets.extend(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Klob_tweets_nrt = []\n",
    "for tweet in Klob_tweets:\n",
    "    if 'RT @' not in tweet['text']:\n",
    "        Klob_tweets_nrt.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For all of the tweets about Sanders:\n",
    "* Load all of the tweets from each week and name\n",
    "* Make a compound list of dictionaries of all the tweets using a for loop and the extend function\n",
    "* Make a list of dictionaries of the tweets that does not include retweets using loop, if statements and append function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sanders1 = load_tweets('data/SandersWk1/@BernieSanders_2020-02-03_to_2020-02-09 (1).json')\n",
    "\n",
    "Sanders2 = load_tweets('data/SandersWk1/bernie_2020-02-03_to_2020-02-09 (1).json')\n",
    "\n",
    "Sanders3 = load_tweets('data/SandersWk2/@BernieSanders_2020-02-10_to_2020-02-16.json')\n",
    "\n",
    "Sanders4 = load_tweets('data/SandersWk2/bernie_2020-02-10_to_2020-02-16.json')\n",
    "\n",
    "Sanders5 = load_tweets('data/SandersWk3/@BernieSanders_2020-02-17_to_2020-02-23.json')\n",
    "\n",
    "Sanders6 = load_tweets('data/SandersWk3/bernie_2020-02-17_to_2020-02-23.json')\n",
    "\n",
    "Sanders7 = load_tweets('data/SandersWk4/@BernieSanders_2020-02-24_to_2020-03-01.json')\n",
    "\n",
    "Sanders8 = load_tweets('data/SandersWk4/bernie_2020-02-24_to_2020-03-01.json')\n",
    "\n",
    "Sanders9 = load_tweets('data/SandersWk5/@BernieSanders_2020-03-02_to_2020-03-05.json')\n",
    "\n",
    "Sanders10 = load_tweets('data/SandersWk5/bernie_2020-03-02_to_2020-03-05.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sanders_tweets = []\n",
    "for tweets in (Sanders1, Sanders2, Sanders3, Sanders4, Sanders5, Sanders6, Sanders7, Sanders8, Sanders9, Sanders10):\n",
    "    Sanders_tweets.extend(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sanders_tweets_nrt = []\n",
    "for tweet in Sanders_tweets:\n",
    "    if 'RT @' not in tweet['text']:\n",
    "        Sanders_tweets_nrt.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For all of the tweets about Warren:\n",
    "* Load all of the tweets from each week and name\n",
    "* Make a compound list of dictionaries of all the tweets using a for loop and the extend function\n",
    "* Make a list of dictionaries of the tweets that does not include retweets using loop, if statements and append function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Warren1 = load_tweets('data/WarrenWk1/@ewarren_2020-02-03_to_2020-02-09 (1).json')\n",
    "\n",
    "Warren2 = load_tweets('data/WarrenWk1/warren_2020-02-03_to_2020-02-09 (1).json')\n",
    "\n",
    "Warren3 = load_tweets('data/WarrenWk2/@ewarren_2020-02-10_to_2020-02-16.json')\n",
    "\n",
    "Warren4 = load_tweets('data/WarrenWk2/warren_2020-02-10_to_2020-02-16.json')\n",
    "\n",
    "Warren5 = load_tweets('data/WarrenWk3/@ewarren_2020-02-17_to_2020-02-23.json')\n",
    "\n",
    "Warren6 = load_tweets('data/WarrenWk3/warren_2020-02-17_to_2020-02-23.json')\n",
    "\n",
    "Warren7 = load_tweets('data/WarrenWk4/@ewarren_2020-02-24_to_2020-03-01.json')\n",
    "\n",
    "Warren8 = load_tweets('data/WarrenWk4/warren_2020-02-24_to_2020-03-01.json')\n",
    "\n",
    "Warren9 = load_tweets('data/WarrenWk5/@ewarren_2020-03-02_to_2020-03-05.json')\n",
    "\n",
    "Warren10 = load_tweets('data/WarrenWk5/warren_2020-03-02_to_2020-03-05.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Warren_tweets = []\n",
    "for tweets in (Warren1, Warren2, Warren3, Warren4, Warren5, Warren6, Warren7, Warren8, Warren9, Warren10):\n",
    "    Warren_tweets.extend(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Warren_tweets_nrt = []\n",
    "for tweet in Warren_tweets:\n",
    "    if 'RT @' not in tweet['text']:\n",
    "        Warren_tweets_nrt.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the data that was cleaned and make into neat files to be able to transfer to other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('move_data/corpus_data1.json', 'w') as out:\n",
    "     out.write(json.dumps(Biden_tweets_nrt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('move_data/corpus_data2.json', 'w') as out:\n",
    "     out.write(json.dumps(Klob_tweets_nrt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('move_data/corpus_data3.json', 'w') as out:\n",
    "     out.write(json.dumps(Sanders_tweets_nrt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('move_data/corpus_data4.json', 'w') as out:\n",
    "     out.write(json.dumps(Warren_tweets_nrt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
