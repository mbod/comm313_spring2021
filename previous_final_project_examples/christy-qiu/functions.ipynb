{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import configparser\n",
    "import os\n",
    "import json\n",
    "import GetOldTweets3 as got\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "import random\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import csv\n",
    "import seaborn as sn\n",
    "import graphviz\n",
    "import math\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyness(fdist1, fdist2, fthreshold=5, keyness_threshold=6.6, top=100, print_table=True):\n",
    "    '''create a keyness comparison table from two frequency lists\n",
    "    '''\n",
    "    \n",
    "    c1size = sum(fdist1.values())\n",
    "    c2size = sum(fdist2.values())\n",
    "\n",
    "    \n",
    "    kdata = []\n",
    "    \n",
    "    for item, freq in fdist1.items():\n",
    "        if freq<fthreshold:\n",
    "            continue\n",
    "            \n",
    "        ref_freq = fdist2.get(item,0)\n",
    "        \n",
    "        if ref_freq<fthreshold:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        keyness = log_likelihood(freq, c1size, ref_freq, c2size)\n",
    "        \n",
    "        row = {'item': item, 'freq': freq, 'ref_freq': ref_freq, 'keyness': keyness}\n",
    "        \n",
    "        if keyness>keyness_threshold:\n",
    "        \n",
    "            kdata.append(row)\n",
    "        \n",
    "    \n",
    "    kdf = pd.DataFrame(kdata)[['item', 'freq', 'ref_freq', 'keyness']]\n",
    "    \n",
    "    kdf=kdf.sort_values('keyness', ascending=False)\n",
    "    \n",
    "    if not print_table:\n",
    "        return kdf[:top]\n",
    "    \n",
    "    template = \"{: <25}{: <10}{: <10}{:0.3f}\"\n",
    "    \n",
    "    header = \"{: <25}{: <10}{: <10}{}\".format('WORD', 'Corpus A Freq.', 'Corpus B Freq.', 'Keyness')\n",
    "    \n",
    "    print(\"{}\\n{}\".format(header, \"=\"*len(header)))\n",
    "    \n",
    "    for item, freq, ref_freq, keyness in kdf[:top].values:\n",
    "        print(template.format(item, freq, ref_freq, keyness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(item_A_freq, corpus_A_size, item_B_freq, corpus_B_size):\n",
    "    '''calculate the log likelihood score for a comparison between the frequency of two items\n",
    "    '''\n",
    "    E1 = corpus_A_size*(item_A_freq+item_B_freq) / (corpus_A_size+corpus_B_size)\n",
    "    E2 = corpus_B_size*(item_A_freq+item_B_freq) / (corpus_A_size+corpus_B_size)\n",
    "\n",
    "    G2 = 2*((item_A_freq*math.log(item_A_freq/E1)) + (item_B_freq*math.log(item_B_freq/E2)))\n",
    "    \n",
    "    sign = 1 if (item_A_freq / corpus_A_size) >= (item_B_freq / corpus_B_size) else -1\n",
    "    \n",
    "    return sign*G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lowercase=False, strip_chars=''):\n",
    "    '''create a list of tokens from a string by splitting on whitespace and applying optional normalization \n",
    "    \n",
    "    Args:\n",
    "        text        -- a string object containing the text to be tokenized\n",
    "        lowercase   -- should text string be normalized as lowercase (default: False)\n",
    "        strip_chars -- a string indicating characters to strip out of text, e.g. punctuation (default: empty string) \n",
    "        \n",
    "    Return:\n",
    "        A list of tokens\n",
    "    '''\n",
    "    \n",
    "    # create a replacement dictionary from the\n",
    "    # string of characters in the **strip_chars**\n",
    "    rdict = str.maketrans('','',strip_chars)\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    tokens = text.translate(rdict).split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_tokens(tokens, n=1):\n",
    "    '''create a list of n-gram tokens from a list of tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens -- a list of tokens\n",
    "        n      -- the size of the window to use to build n-gram token list\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "        list of n-gram strings (whitespace separated) of length n\n",
    "    '''\n",
    "    \n",
    "    if n<2 or n>len(tokens):\n",
    "        return tokens\n",
    "    \n",
    "    new_tokens = []\n",
    "    \n",
    "    for i in range(len(tokens)-n+1):\n",
    "        new_tokens.append(\" \".join(tokens[i:i+n]))\n",
    "        \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kwic(kw, text, win=4):\n",
    "    '''A basic KWIC function for a text\n",
    "    \n",
    "    Args:\n",
    "        kw   -- string match for keyword to match for each line\n",
    "        text -- a list of tokens for the text\n",
    "        \n",
    "    Return:\n",
    "        list of lines of form [ [left context words], kw, [right context words]]\n",
    "    '''\n",
    "    \n",
    "    hits = [(w,i) for i,w in enumerate(text) if w==kw]\n",
    "    \n",
    "    lines = []\n",
    "    for hit in hits:\n",
    "        left = text[hit[1]-win:hit[1]]\n",
    "        kw = text[hit[1]]\n",
    "        right = text[hit[1]+1 : hit[1]+win+1]\n",
    "        \n",
    "        \n",
    "        left = ['']*(win-len(left)) + left if len(left)<win else left\n",
    "        right = right+['']*(win-len(right)) if len(right)<win else right\n",
    "\n",
    "        \n",
    "        lines.append([left, kw, right])\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shared_vocab(cdf, start=5, end=100):\n",
    "    \n",
    "    def scaler(values):\n",
    "        vmin=min(values)\n",
    "        vmax=max(values)\n",
    "\n",
    "        scaled_values = [(v-vmin)/(vmax-vmin)for v in values]\n",
    "        return scaled_values\n",
    "    \n",
    "    with plt.style.context('seaborn-paper'):\n",
    "\n",
    "        fig = plt.figure(figsize=(18,9))\n",
    "\n",
    "        ax_max = cdf.iloc[start:end][['Tweets_percent','Articles_percent']].max().max()\n",
    "        ax_min = cdf.iloc[start:end][['Tweets_percent','Articles_percent']].min().min()\n",
    "        \n",
    "        cdf['text_size'] = scaler(cdf.Tweets_percent.add(cdf.Articles_percent))\n",
    "\n",
    "        for row in cdf[start:end].itertuples():\n",
    "            plt.text(row.Tweets_percent*.8, row.Articles_percent*1.2, row.word, \n",
    "                     color='#0000FF', \n",
    "                     size=140*row.text_size,\n",
    "                     ha='center', va='center', alpha=0.25)\n",
    "\n",
    "        plt.axis([0, ax_max, 0, ax_max])\n",
    "        plt.plot((0,ax_max),(0,ax_max)) #, color='#A0A0A0')\n",
    "\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel('Use in Tweets (% of tokens)', fontsize=20)\n",
    "        plt.ylabel('Use in Articles (% of tokens)', fontsize=20)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kwic(kwic, win=None):\n",
    "    '''A basic print function for a KWIC object\n",
    "    \n",
    "    Args:\n",
    "        kwic -- a list of KWIC lines of the form [ [left words], kw, [right words]]\n",
    "        win  -- if None then use all words provided in context otherwise limit by win\n",
    "        \n",
    "    Prints KWIC lines with left context width/padding win*8 characters\n",
    "    '''\n",
    "    \n",
    "    if not kwic:\n",
    "        return\n",
    "    \n",
    "    if win is None:\n",
    "        win = len(kwic[0][0])\n",
    "    \n",
    "    for line in kwic:\n",
    "        print(\"{: >{}}  {}  {}\".format(' '.join(line[0][-win:]), \n",
    "                                      win*10, \n",
    "                                      line[1], \n",
    "                                      ' '.join(line[2][:win])\n",
    "                                     )\n",
    "             )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_kwic(kwic, order=None):\n",
    "    ''' sort a kwic list using the passed positional arguments \n",
    "    \n",
    "    Args:\n",
    "        kwic   -- a list of lists [ [left tokens], kw, [right tokens]]\n",
    "        order  -- a list of one or more positional arguments of form side-pos, e.g. L1, R3, L4 (default: None)\n",
    "    \n",
    "    Returns:\n",
    "        kwic sorted for each positional argument in reverse, i.e. ['R1','L1'] sorts first by L1 and then R1\n",
    "    '''\n",
    "    if order is None:\n",
    "        return kwic\n",
    "   \n",
    "    order = [order] if not type(order) is list else order\n",
    "    order.reverse()\n",
    "    \n",
    "    for sort_term in order:\n",
    "        if not re.match('[LR][1-4]', sort_term):\n",
    "            pass\n",
    "        \n",
    "        pos1 = 0 if sort_term[0]=='L' else 2\n",
    "        pos2 = int(sort_term[1])-1\n",
    "        pos2 = 3-pos2 if sort_term[0]=='L' else pos2\n",
    "        kwic.sort(key=lambda l : l[pos1][pos2])\n",
    "    \n",
    "    return kwic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collocates(kw,token_dict, span=4):\n",
    "    ''' Create a frequency list of the collocates of a specified keyword in a corpus of texts\n",
    "    \n",
    "    Args:\n",
    "        kw         -- keyword to use as center of analysis\n",
    "        token_dict -- corpus in the form of a dictionary of tokenized texts where a text is a list of tokens\n",
    "        span       --\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        Frequency list of collocates in a Counter object \n",
    "    '''\n",
    "    collocates = Counter()\n",
    "    for speech, tokens in token_dict.items():\n",
    "        hits = [i for i,w in enumerate(tokens) if w==kw]\n",
    "        for i in hits:\n",
    "            collocates.update(tokens[i-span:i] + tokens[i+1:span+1] )\n",
    "        \n",
    "    return collocates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocates(tokens, kw, win=[4,4]):\n",
    "    '''return the collocates in a window around a given keyword\n",
    "    \n",
    "    Args:\n",
    "          tokens -- a list of tokens\n",
    "          kw     -- keyword string to find and get collocates for\n",
    "          win    -- a list of number of tokens to left (index 0) and right (index 1) to use; default: [4,4]\n",
    "    \n",
    "    Returns:\n",
    "          a list of contexts (matching window specification) around each instance of keyword in tokens\n",
    "    '''\n",
    "    hits = [p for p,t in enumerate(tokens) if t==kw]\n",
    "    \n",
    "    context=[]\n",
    "    for hit in hits:\n",
    "        left = [] if win[0]<1 else tokens[hit-win[0]:hit]\n",
    "        right = [] if win[1]<1 else tokens[hit+1:hit+win[1]+1]\n",
    "        \n",
    "        context.extend(left)\n",
    "        context.extend(right)\n",
    "        \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colls(texts,kw, win=[4,4]):\n",
    "    '''create a collocate frequency list for instances of a kw in a list of texts\n",
    "    \n",
    "    Args:\n",
    "        texts  -- a list of tokenized texts\n",
    "        kw     -- keyword string to find and get collocates for\n",
    "        win    -- a list of number of tokens to left (index 0) and right (index 1) to use; default: [4,4]\n",
    "    \n",
    "    Returns:\n",
    "        a list-of-tuples where each tuple is (collocate, freq_with_kw, coll_total_freq)\n",
    "    '''\n",
    "    word_dist = Counter()\n",
    "    colls = Counter()\n",
    "    for text, tokens in texts.items():\n",
    "        word_dist.update(tokens)\n",
    "        colls.update(collocates(tokens,kw, win))\n",
    "    \n",
    "    return [(str(k),v, word_dist[k]) for k,v in colls.items()], word_dist.get(kw), sum(word_dist.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_collocates(kw, collocate_list, num=20, show_freq=False, title=None, threshold=1):\n",
    "    ''' Create a graph of the collocates of a keyword within a specified window and threshold\n",
    "    \n",
    "    Args:\n",
    "        kw              -- keyword to place at center of graph\n",
    "        collocate_list  -- Counter object of collocate frequencies\n",
    "        num             -- the number of collocates (in descending frequency to display) [default=20]\n",
    "        show_freq       -- whether to show frequency beside edge True/False [default=False]\n",
    "        title           -- string to use as a title for the plot [default=None]\n",
    "        threshold       -- frequency threshold for showing edges [default=1]\n",
    "        \n",
    "    '''\n",
    "    cG = graphviz.Graph(engine='neato')\n",
    "    cG.attr('graph', overlap='scalexy', size=\"6,6\")\n",
    "    if title:\n",
    "        cG.attr('graph', label=title, labelloc='t', fontsize='20')\n",
    "    for item, freq in collocate_list.most_common(num):\n",
    "        if freq >= threshold:\n",
    "            cG.edge(kw.upper(), item, penwidth=str(math.log(freq,2)), \n",
    "                    label=None if not show_freq else str(freq))\n",
    "    \n",
    "    return cG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_items(dist1, dist2, dist3, dist4, items, scaling=10000):\n",
    "    ''' given two Counter objects with common keys compare the frequency and relative frequency of list of items\n",
    "    \n",
    "    Args:\n",
    "        dist1    -- Counter frequency list object\n",
    "        dist2    -- Counter frequency list object\n",
    "        items    -- list of string items that should be keys in dist1 and dist2\n",
    "        scaling  -- normalization factor, e.g. 10,000 words (default: 100000)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list of tuples of form\n",
    "            (item, item_freq_dist1, norm_item_freq_dist1, item_freq_dist2, norm_item_freq_dist2)\n",
    "    '''\n",
    "    dist1_size = sum(dist2.values())\n",
    "    dist2_size = sum(dist2.values())\n",
    "    dist3_size = sum(dist3.values())\n",
    "    dist4_size = sum(dist4.values())\n",
    "\n",
    "    item_comparison = []\n",
    "    \n",
    "    for item in items:\n",
    "        \n",
    "        d1_freq = dist1.get(item,0)\n",
    "        d2_freq = dist2.get(item,0)\n",
    "        d3_freq = dist3.get(item,0)\n",
    "        d4_freq = dist4.get(item,0)\n",
    "        \n",
    "        item_comparison.append((item, \n",
    "                                d1_freq, d1_freq/dist1_size*scaling,\n",
    "                                d2_freq, d2_freq/dist2_size*scaling,\n",
    "                                d3_freq, d3_freq/dist3_size*scaling,\n",
    "                                d4_freq, d4_freq/dist4_size*scaling))\n",
    "    \n",
    "    return item_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_2items(dist1, dist2, items, scaling=10000):\n",
    "    ''' given two Counter objects with common keys compare the frequency and relative frequency of list of items\n",
    "    \n",
    "    Args:\n",
    "        dist1    -- Counter frequency list object\n",
    "        dist2    -- Counter frequency list object\n",
    "        items    -- list of string items that should be keys in dist1 and dist2\n",
    "        scaling  -- normalization factor, e.g. 10,000 words (default: 100000)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list of tuples of form\n",
    "            (item, item_freq_dist1, norm_item_freq_dist1, item_freq_dist2, norm_item_freq_dist2)\n",
    "    '''\n",
    "    dist1_size = sum(dist2.values())\n",
    "    dist2_size = sum(dist2.values())\n",
    "\n",
    "    item_comparison = []\n",
    "    \n",
    "    for item in items:\n",
    "        \n",
    "        d1_freq = dist1.get(item,0)\n",
    "        d2_freq = dist2.get(item,0)\n",
    "        \n",
    "        item_comparison.append((item, \n",
    "                                d1_freq, d1_freq/dist1_size*scaling,\n",
    "                                d2_freq, d2_freq/dist2_size*scaling))\n",
    "    \n",
    "    return item_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(comparison_data, label1='corpus 1', label2='corpus 2',label3='corpus 3',label4='corpus 4'):\n",
    "    ''' create a paired barplot of relative frequencies of items in two corpora\n",
    "    \n",
    "    Args:\n",
    "        comparison_data --  list of tuples produced by the compare_items() function\n",
    "        label1          --  legend label for first corpus (default: corpus 1)\n",
    "        label2          --  legend label for second corpus (default: corpus 2)\n",
    "        \n",
    "    Produces a Seaborn barplot\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    \n",
    "    df=pd.DataFrame(comparison_data)[[0,2,4,6,8]] \n",
    "    df.columns = ['item', label1, label2, label3, label4]\n",
    "    df2=df.melt(id_vars=['item'])\n",
    "    df2.columns=['item', 'corpus', 'frequency']\n",
    "    sn.barplot(x='item',y='frequency', hue='corpus',data=df2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison2_plot(comparison_data, label1='corpus 1', label2='corpus 2'):\n",
    "    ''' create a paired barplot of relative frequencies of items in two corpora\n",
    "    \n",
    "    Args:\n",
    "        comparison_data --  list of tuples produced by the compare_items() function\n",
    "        label1          --  legend label for first corpus (default: corpus 1)\n",
    "        label2          --  legend label for second corpus (default: corpus 2)\n",
    "        \n",
    "    Produces a Seaborn barplot\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    \n",
    "    df=pd.DataFrame(comparison_data)[[0,2,4]] \n",
    "    df.columns = ['item', label1, label2]\n",
    "    df2=df.melt(id_vars=['item'])\n",
    "    df2.columns=['item', 'corpus', 'frequency']\n",
    "    sn.barplot(x='item',y='frequency', hue='corpus',data=df2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonconverter(o):\n",
    "    if isinstance(o, datetime.datetime):\n",
    "        return o.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_query_tweets(query, date_since, date_until, max=100000):\n",
    "    print(f\"Downloading tweets for query: '{query}' from {date_since} to {date_until}\")\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query)\\\n",
    "                                               .setSince(date_since)\\\n",
    "                                               .setUntil(date_until)\\\n",
    "                                               .setMaxTweets(max)\n",
    "\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    list_of_tweets = [tweet.__dict__ for tweet in tweets]\n",
    "    return list_of_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_user_tweets(username, user_query, date_since, date_until):\n",
    "    print(f\"Downloading for {username} and {query}\")\n",
    "    tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
    "                                               .setQuerySearch(query)\\\n",
    "                                               .setSince(date_since)\\\n",
    "                                               .setUntil(date_until)\n",
    "\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    list_of_tweets = [tweet.__dict__ for tweet in tweets]\n",
    "    return list_of_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(tf):\n",
    "    tweets = []\n",
    "    for tweet in open(tf):\n",
    "        tweets.append(json.loads(tweet))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DictListUpdate(lis1, lis2):\n",
    "    for aLis1 in lis1:\n",
    "        lis2.append(aLis1)\n",
    "    return lis2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
