{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's in this notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I've included everything that the code in the blog post will require to run successfully. I did so in an effort to reduce the amount of code in the final blog post. This notebook pulls from the analysis1.ipynb and the analysis2.ipynb notesbooks, which were too large in size to run efficiently in the final blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import configparser\n",
    "import os\n",
    "import json\n",
    "import GetOldTweets3 as got\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "import random\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_tweets = load_tweets('data/1/tweets_2020-01-01_to_2020-02-01.json')\n",
    "feb_tweets = load_tweets('data/2/tweets_2020-02-01_to_2020-03-01.json')\n",
    "mar_tweets = load_tweets('data/3/tweets_2020-03-01_to_2020-04-01.json')\n",
    "apr_tweets = load_tweets('data/4/tweets_2020-04-01_to_2020-05-01.json')\n",
    "all_time = load_tweets('data/all_time/tweets_2020-01-01_to_2020-05-01.json')\n",
    "trump_tweets = load_tweets('data/all_time/realdonaldtrump_2020-01-01_to_2020-05-01.json')\n",
    "pompeo_tweets = load_tweets('data/all_time/secpompeo_2020-01-01_to_2020-05-01.json')\n",
    "racist_tweets = load_tweets('data/all_time/racist_tweets_2020-01-01_to_2020-05-01.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = json.load(open('data/corpus_index1.json'))\n",
    "corpus2 = json.load(open('data/corpus_index2.json'))\n",
    "corpus3 = json.load(open('data/corpus_index3.json'))\n",
    "corpus4 = json.load(open('data/corpus_index4.json'))\n",
    "corp_all = json.load(open('data/corpus_index_all.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_t_word_dist=Counter()\n",
    "all_t_bigram_dist=Counter()\n",
    "all_t_trigram_dist=Counter()\n",
    "\n",
    "all_t_tokens = []\n",
    "for tweet in all_time:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    all_t_tokens.extend(toks)\n",
    "\n",
    "all_t_bigrams=get_ngram_tokens(all_t_tokens,2)\n",
    "all_t_trigrams=get_ngram_tokens(all_t_tokens,3)\n",
    "\n",
    "all_t_word_dist.update(all_t_tokens)\n",
    "all_t_bigram_dist.update(all_t_bigrams)\n",
    "all_t_trigram_dist.update(all_t_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['asianamerican', 'asian', 'american', \\\n",
    "            'racism', 'racist', 'xenophobia', 'racism', 'racist', 'xenophobia', \\\n",
    "            'coronavirus', 'corona virus', 'covid19', 'covid 19', 'pandemic', 'virus', \"chinese virus\", \"china virus\", \\\n",
    "            'coronavirus', 'covid19', 'pandemic', 'chinavirus', 'chinesevirus']\n",
    "stripped_tweets_tokens = all_t_tokens\n",
    "words_to_remove= stopwords.words('english')+queries\n",
    "for tweet in list(stripped_tweets_tokens):\n",
    "    if tweet in words_to_remove:\n",
    "        stripped_tweets_tokens.remove(tweet)\n",
    "\n",
    "stripped_tweets_tokens = [x for x in stripped_tweets_tokens if not x.startswith('https')]\n",
    "stripped_tweets_wfreq = Counter(stripped_tweets_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Bi/Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**January**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_t_word_dist=Counter()\n",
    "jan_t_bigram_dist=Counter()\n",
    "jan_t_trigram_dist=Counter()\n",
    "\n",
    "jan_t_tokens = []\n",
    "for tweet in jan_tweets:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    jan_t_tokens.extend(toks)\n",
    "\n",
    "jan_t_bigrams=get_ngram_tokens(jan_t_tokens,2)\n",
    "jan_t_trigrams=get_ngram_tokens(jan_t_tokens,3)\n",
    "\n",
    "jan_t_word_dist.update(jan_t_tokens)\n",
    "jan_t_bigram_dist.update(jan_t_bigrams)\n",
    "jan_t_trigram_dist.update(jan_t_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**February**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_t_word_dist=Counter()\n",
    "feb_t_bigram_dist=Counter()\n",
    "feb_t_trigram_dist=Counter()\n",
    "\n",
    "feb_t_tokens = []\n",
    "for tweet in feb_tweets:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    feb_t_tokens.extend(toks)\n",
    "\n",
    "feb_t_bigrams=get_ngram_tokens(feb_t_tokens,2)\n",
    "feb_t_trigrams=get_ngram_tokens(feb_t_tokens,3)\n",
    "\n",
    "feb_t_word_dist.update(feb_t_tokens)\n",
    "feb_t_bigram_dist.update(feb_t_bigrams)\n",
    "feb_t_trigram_dist.update(feb_t_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_top_20_bigrams = feb_t_bigram_dist.most_common(20)\n",
    "feb_top_20_trigrams = feb_t_trigram_dist.most_common(20)\n",
    "feb_bigram_df = pd.DataFrame(feb_top_20_bigrams, columns = ['Bigram','Freq'])\n",
    "feb_bigram_list = list(feb_bigram_df['Bigram'])\n",
    "feb_trigram_df = pd.DataFrame(feb_top_20_trigrams, columns = ['Trigram','Freq'])\n",
    "feb_trigram_list = list(feb_trigram_df['Trigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = list(range(1, 21))\n",
    "feb_bitrigram = pd.DataFrame(rank, columns = ['Rank'])\n",
    "feb_bitrigram['Bigram']=feb_bigram_list\n",
    "feb_bitrigram['Trigram']=feb_trigram_list\n",
    "feb_bitrigram.set_index('Rank', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar_t_word_dist=Counter()\n",
    "mar_t_bigram_dist=Counter()\n",
    "mar_t_trigram_dist=Counter()\n",
    "\n",
    "mar_t_tokens = []\n",
    "for tweet in mar_tweets:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    mar_t_tokens.extend(toks)\n",
    "\n",
    "mar_t_bigrams=get_ngram_tokens(mar_t_tokens,2)\n",
    "mar_t_trigrams=get_ngram_tokens(mar_t_tokens,3)\n",
    "\n",
    "mar_t_word_dist.update(mar_t_tokens)\n",
    "mar_t_bigram_dist.update(mar_t_bigrams)\n",
    "mar_t_trigram_dist.update(mar_t_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar_top_20_bigrams = mar_t_bigram_dist.most_common(20)\n",
    "mar_top_20_trigrams = mar_t_trigram_dist.most_common(20)\n",
    "mar_bigram_df = pd.DataFrame(mar_top_20_bigrams, columns = ['Bigram','Freq'])\n",
    "mar_bigram_list = list(mar_bigram_df['Bigram'])\n",
    "mar_trigram_df = pd.DataFrame(mar_top_20_trigrams, columns = ['Trigram','Freq'])\n",
    "mar_trigram_list = list(mar_trigram_df['Trigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = list(range(1, 21))\n",
    "mar_bitrigram = pd.DataFrame(rank, columns = ['Rank'])\n",
    "mar_bitrigram['Bigram']=mar_bigram_list\n",
    "mar_bitrigram['Trigram']=mar_trigram_list\n",
    "mar_bitrigram.set_index('Rank', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### April"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_t_word_dist=Counter()\n",
    "apr_t_bigram_dist=Counter()\n",
    "apr_t_trigram_dist=Counter()\n",
    "\n",
    "apr_t_tokens = []\n",
    "for tweet in apr_tweets:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    apr_t_tokens.extend(toks)\n",
    "\n",
    "apr_t_bigrams=get_ngram_tokens(apr_t_tokens,2)\n",
    "apr_t_trigrams=get_ngram_tokens(apr_t_tokens,3)\n",
    "\n",
    "apr_t_word_dist.update(apr_t_tokens)\n",
    "apr_t_bigram_dist.update(apr_t_bigrams)\n",
    "apr_t_trigram_dist.update(apr_t_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_top_20_bigrams = apr_t_bigram_dist.most_common(20)\n",
    "apr_top_20_trigrams = apr_t_trigram_dist.most_common(20)\n",
    "apr_bigram_df = pd.DataFrame(apr_top_20_bigrams, columns = ['Bigram','Freq'])\n",
    "apr_bigram_list = list(apr_bigram_df['Bigram'])\n",
    "apr_trigram_df = pd.DataFrame(apr_top_20_trigrams, columns = ['Trigram','Freq'])\n",
    "apr_trigram_list = list(apr_trigram_df['Trigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = list(range(1, 21))\n",
    "apr_bitrigram = pd.DataFrame(rank, columns = ['Rank'])\n",
    "apr_bitrigram['Bigram']=apr_bigram_list\n",
    "apr_bitrigram['Trigram']=apr_trigram_list\n",
    "apr_bitrigram.set_index('Rank', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article distribution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_word_dist=Counter()\n",
    "jan_bigram_dist=Counter()\n",
    "jan_trigram_dist=Counter()\n",
    "\n",
    "for article in corpus1:\n",
    "    filename = article['Filename']\n",
    "    text = open('data/text1/{}'.format(filename)).read()\n",
    "    article['text'] = text\n",
    "    \n",
    "    jan_tokens = tokenize(article['text'], lowercase=True, strip_chars=string.punctuation)\n",
    "    \n",
    "    article['tokens'] = jan_tokens \n",
    "    article['token_cnt'] = len(jan_tokens)\n",
    "    article['type_cnt'] = len(set(jan_tokens))\n",
    "    \n",
    "    jan_tokens = article['tokens']\n",
    "    jan_bigrams=get_ngram_tokens(jan_tokens,2)\n",
    "    jan_trigrams=get_ngram_tokens(jan_tokens,3)\n",
    "    \n",
    "    jan_word_dist.update(jan_tokens)\n",
    "    jan_bigram_dist.update(jan_bigrams)\n",
    "    jan_trigram_dist.update(jan_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_word_dist=Counter()\n",
    "feb_bigram_dist=Counter()\n",
    "feb_trigram_dist=Counter()\n",
    "feb_all_tokens = []\n",
    "for article in corpus2:\n",
    "    filename = article['Filename']\n",
    "    text = open('data/text2/{}'.format(filename)).read()\n",
    "    article['text'] = text\n",
    "    \n",
    "    feb_tokens = tokenize(article['text'], lowercase=True, strip_chars=string.punctuation)\n",
    "    feb_all_tokens.extend(feb_tokens)\n",
    "    \n",
    "    article['tokens'] = feb_tokens \n",
    "    article['token_cnt'] = len(feb_tokens)\n",
    "    article['type_cnt'] = len(set(feb_tokens))\n",
    "    \n",
    "    feb_tokens = article['tokens']\n",
    "    feb_bigrams=get_ngram_tokens(feb_tokens,2)\n",
    "    feb_trigrams=get_ngram_tokens(feb_tokens,3)\n",
    "    \n",
    "    feb_word_dist.update(feb_tokens)\n",
    "    feb_bigram_dist.update(feb_bigrams)\n",
    "    feb_trigram_dist.update(feb_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mar_word_dist=Counter()\n",
    "mar_bigram_dist=Counter()\n",
    "mar_trigram_dist=Counter()\n",
    "\n",
    "for article in corpus3:\n",
    "    filename = article['Filename']\n",
    "    text = open('data/text3/{}'.format(filename)).read()\n",
    "    article['text'] = text\n",
    "    \n",
    "    mar_tokens = tokenize(article['text'], lowercase=True, strip_chars=string.punctuation)\n",
    "    \n",
    "    article['tokens'] = mar_tokens \n",
    "    article['token_cnt'] = len(mar_tokens)\n",
    "    article['type_cnt'] = len(set(mar_tokens))\n",
    "    \n",
    "    mar_tokens = article['tokens']\n",
    "    mar_bigrams=get_ngram_tokens(mar_tokens,2)\n",
    "    mar_trigrams=get_ngram_tokens(mar_tokens,3)\n",
    "    \n",
    "    mar_word_dist.update(mar_tokens)\n",
    "    mar_bigram_dist.update(mar_bigrams)\n",
    "    mar_trigram_dist.update(mar_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr_word_dist=Counter()\n",
    "apr_bigram_dist=Counter()\n",
    "apr_trigram_dist=Counter()\n",
    "\n",
    "for article in corpus4:\n",
    "    filename = article['Filename']\n",
    "    text = open('data/text4/{}'.format(filename)).read()\n",
    "    article['text'] = text\n",
    "    \n",
    "    apr_tokens = tokenize(article['text'], lowercase=True, strip_chars=string.punctuation)\n",
    "    \n",
    "    article['tokens'] = apr_tokens \n",
    "    article['token_cnt'] = len(apr_tokens)\n",
    "    article['type_cnt'] = len(set(apr_tokens))\n",
    "    \n",
    "    apr_tokens = article['tokens']\n",
    "    apr_bigrams=get_ngram_tokens(apr_tokens,2)\n",
    "    apr_trigrams=get_ngram_tokens(apr_tokens,3)\n",
    "    \n",
    "    apr_word_dist.update(apr_tokens)\n",
    "    apr_bigram_dist.update(apr_bigrams)\n",
    "    apr_trigram_dist.update(apr_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_tweets = DictListUpdate(mar_tweets,apr_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_tweets_tokens = []\n",
    "for tweet in recent_tweets:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    recent_tweets_tokens.extend(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_recenttweets_tokens = recent_tweets_tokens\n",
    "words_to_remove= stopwords.words('english')+queries\n",
    "for tweet in list(stripped_recenttweets_tokens):\n",
    "    if tweet in words_to_remove:\n",
    "        stripped_recenttweets_tokens.remove(tweet)\n",
    "\n",
    "stripped_recenttweets_tokens = [x for x in stripped_recenttweets_tokens if not x.startswith('https')]\n",
    "stripped_recenttweets_wfreq = Counter(stripped_recenttweets_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tweets = DictListUpdate(jan_tweets,feb_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tweets_tokens = []\n",
    "for tweet in old_tweets:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    old_tweets_tokens.extend(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_oldtweets_tokens = old_tweets_tokens\n",
    "words_to_remove= stopwords.words('english')+queries\n",
    "for tweet in list(stripped_oldtweets_tokens):\n",
    "    if tweet in words_to_remove:\n",
    "        stripped_oldtweets_tokens.remove(tweet)\n",
    "\n",
    "stripped_oldtweets_tokens = [x for x in stripped_oldtweets_tokens if not x.startswith('https')]\n",
    "stripped_oldtweets_wfreq = Counter(stripped_oldtweets_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_size = len(stripped_oldtweets_tokens)\n",
    "recent_size = len(stripped_recenttweets_tokens)\n",
    "top_old = stripped_oldtweets_wfreq.most_common(30)\n",
    "top_recent = stripped_recenttweets_wfreq.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trump and Pompeo's Tweets Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Counter(tweet['date'][:10] for tweet in all_time)\n",
    "dftweets_raw = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "dftweets_cleaned = dftweets_raw.rename(columns = {\"index\": \"date\", 0: \"count\"})\n",
    "dftweets = dftweets_cleaned.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Counter(article['Date'] for article in corp_all)\n",
    "dflexis_raw = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "dflexis_cleaned = dflexis_raw.rename(columns = {\"index\": \"date\", 0: \"count\"})\n",
    "dflexis = dflexis_cleaned.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Counter(tweet['date'][:10] for tweet in trump_tweets)\n",
    "dftrump_raw = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "dftrump_cleaned = dftrump_raw.rename(columns = {\"index\": \"date\", 0: \"count\"})\n",
    "dftrump = dftrump_cleaned.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Counter(tweet['date'][:10] for tweet in pompeo_tweets)\n",
    "dfpompeo_raw = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "dfpompeo_cleaned = dfpompeo_raw.rename(columns = {\"index\": \"date\", 0: \"count\"})\n",
    "dfpompeo = dfpompeo_cleaned.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Counter(tweet['date'][:10] for tweet in racist_tweets)\n",
    "dfracist_tweets_raw = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "dfracist_tweets_cleaned = dfracist_tweets_raw.rename(columns = {\"index\": \"date\", 0: \"count\"})\n",
    "dfracist_tweets = dfracist_tweets_cleaned.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silent Tweets vs. Discussion Creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "silent_tweets = []\n",
    "for tweet in all_time:\n",
    "    if tweet['replies']==0:\n",
    "        silent_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_silent_tokens = []\n",
    "for tweet in silent_tweets:\n",
    "    text = tweet['text']\n",
    "    toks = tokenize(text, lowercase=True, strip_chars='')\n",
    "    raw_silent_tokens.extend(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "silent_tokens = []\n",
    "for tweet in silent_tweets:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    silent_tokens.extend(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_silent_tokens = silent_tokens\n",
    "words_to_remove= stopwords.words('english')+queries\n",
    "for tweet in list(stripped_silent_tokens):\n",
    "    if tweet in words_to_remove:\n",
    "        stripped_silent_tokens.remove(tweet)\n",
    "\n",
    "stripped_silent_tokens = [x for x in stripped_silent_tokens if not x.startswith('https')]\n",
    "stripped_silent_wfreq = Counter(stripped_silent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_creators = []\n",
    "for tweet in all_time:\n",
    "    if tweet['replies']>0:\n",
    "        discussion_creators.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_discussion_tokens = []\n",
    "for tweet in discussion_creators:\n",
    "    text = tweet['text']\n",
    "    toks = tokenize(text, lowercase=True, strip_chars='')\n",
    "    raw_discussion_tokens.extend(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_tokens = []\n",
    "for tweet in discussion_creators:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    discussion_tokens.extend(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_discussion_tokens = discussion_tokens\n",
    "words_to_remove= stopwords.words('english')+queries\n",
    "for tweet in list(stripped_discussion_tokens):\n",
    "    if tweet in words_to_remove:\n",
    "        stripped_discussion_tokens.remove(tweet)\n",
    "\n",
    "stripped_discussion_tokens = [x for x in stripped_discussion_tokens if not x.startswith('https')]\n",
    "stripped_discussion_wfreq = Counter(stripped_discussion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared vocab and word freq. visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_dist=Counter()\n",
    "all_bigram_dist=Counter()\n",
    "all_trigram_dist=Counter()\n",
    "\n",
    "all_tokens = []\n",
    "for article in corp_all:\n",
    "    filename = article['Filename']\n",
    "    text = open('data/text_all/{}'.format(filename)).read()\n",
    "    tokens = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    all_tokens.extend(tokens)\n",
    "    \n",
    "    article['token_cnt'] = len(tokens)\n",
    "    article['type_cnt'] = len(set(tokens))\n",
    "    \n",
    "    bigrams=get_ngram_tokens(tokens,2)\n",
    "    trigrams=get_ngram_tokens(tokens,3)\n",
    "    \n",
    "    all_word_dist.update(tokens)\n",
    "    all_bigram_dist.update(bigrams)\n",
    "    all_trigram_dist.update(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens1 = all_tokens[:145449]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens2 = all_tokens[145449:290898]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens3 = all_tokens[290898:436347]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens4 = all_tokens[436347:581796]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens5 = all_tokens[581796:727245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens6 = all_tokens[727245:872694]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens7 = all_tokens[872694:1018143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens8 = all_tokens[1018143:1163592]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens9 = all_tokens[1163592:1309041]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens10 = all_tokens[1309041:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['asianamerican', 'asian', 'american', \\\n",
    "            'racism', 'racist', 'xenophobia', 'racism', 'racist', 'xenophobia', \\\n",
    "            'coronavirus', 'corona virus', 'covid19', 'covid 19', 'pandemic', 'virus', \"chinese virus\", \"china virus\", \\\n",
    "            'coronavirus', 'covid19', 'pandemic', 'chinavirus', 'chinesevirus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens1:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens1.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens2:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens2.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens3:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens3.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens4:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens4.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens5:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens5.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens6:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens6.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens7:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens7.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens8:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens8.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens9:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens9.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove= stopwords.words('english')+queries\n",
    "for article_word in s_all_tokens10:\n",
    "    if article_word in words_to_remove:\n",
    "        s_all_tokens10.remove(article_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens = s_all_tokens1+s_all_tokens2+s_all_tokens3+s_all_tokens4+s_all_tokens5+s_all_tokens6+s_all_tokens7+s_all_tokens8+s_all_tokens9+s_all_tokens10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_all_tokens_dist = Counter(s_all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_shared_items = [(item, value, s_all_tokens_dist.get(item)) \n",
    "                for item,value in stripped_tweets_wfreq.items() if s_all_tokens_dist.get(item)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweet_num_tokens = sum(stripped_tweets_wfreq.values())\n",
    "all_article_num_tokens = sum(s_all_tokens_dist.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_shared_items.sort(key=lambda i: i[1]+i[2], reverse=True)\n",
    "cdf=pd.DataFrame(all_shared_items, columns=['word','Number of occurrences in Tweets','Number of occurrences in articles'])\n",
    "cdf['Tweets_percent']=cdf['Number of occurrences in Tweets'] / all_tweet_num_tokens * 100\n",
    "cdf['Articles_percent']=cdf['Number of occurrences in articles'] / all_article_num_tokens * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Community\" and \"Health\" Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_community_colls = Counter()\n",
    "article_community_colls.update(collocates(all_tokens, 'community',win=[5,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_community_colls = Counter()\n",
    "tweet_community_colls.update(collocates(stripped_tweets_tokens, 'community',win=[5,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_health_colls = Counter()\n",
    "article_health_colls.update(collocates(all_tokens, 'health',win=[5,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_health_colls = Counter()\n",
    "tweet_health_colls.update(collocates(stripped_tweets_tokens, 'health',win=[5,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweets**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sid_scores=[]\n",
    "\n",
    "for tweet in all_time:\n",
    "    scores = sid.polarity_scores(tweet['text'])\n",
    "    scores['text']=tweet['text']\n",
    "    scores['date']=tweet['date']\n",
    "    tweet_sid_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_monthdate = {}\n",
    "for tweet in tweet_sid_scores:\n",
    "    ymd = tweet['date'][:10]\n",
    "    try:\n",
    "        by_monthdate[ymd].append(tweet['compound'])\n",
    "    except:\n",
    "        by_monthdate[ymd] = [tweet['compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets = [{ 'date': y, 'avg_sent': sum(d)/len(d)}\n",
    "        for y, d in by_monthdate.items() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sid_scores=[]\n",
    "\n",
    "for article in corp_all:\n",
    "    filename = article['Filename']\n",
    "    text = open('data/text_all/{}'.format(filename)).read()\n",
    "    article['text']=text\n",
    "    scores = sid.polarity_scores(article['text'])\n",
    "    scores['text']=article['text']\n",
    "    scores['date']=article['Date']\n",
    "    article_sid_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_by_monthdate = {}\n",
    "for article in article_sid_scores:\n",
    "    ymd = article['date']\n",
    "    try:\n",
    "        a_by_monthdate[ymd].append(article['compound'])\n",
    "    except:\n",
    "        a_by_monthdate[ymd] = [article['compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_articles = [{ 'date': y, 'avg_sent': sum(d)/len(d)}\n",
    "        for y, d in a_by_monthdate.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(data_tweets)\n",
    "a = pd.DataFrame(data_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Racist Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_racist_tokens = []\n",
    "for tweet in racist_tweets:\n",
    "    text = tweet['text']\n",
    "    toks = tokenize(text, lowercase=False, strip_chars='')\n",
    "    raw_racist_tokens.extend(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "racist_word_dist=Counter()\n",
    "racist_bigram_dist=Counter()\n",
    "racist_trigram_dist=Counter()\n",
    "\n",
    "racist_tokens = []\n",
    "for tweet in racist_tweets:\n",
    "    text = tweet['text'].replace('&amp;', '&').replace('”', '').replace('\\'', '').replace('’', '').replace('“', '')\n",
    "    toks = tokenize(text, lowercase=True, strip_chars=string.punctuation)\n",
    "    racist_tokens.extend(toks)\n",
    "\n",
    "racist_bigrams=get_ngram_tokens(racist_tokens,2)\n",
    "racist_trigrams=get_ngram_tokens(racist_tokens,3)\n",
    "\n",
    "racist_word_dist.update(racist_tokens)\n",
    "racist_bigram_dist.update(racist_bigrams)\n",
    "racist_trigram_dist.update(racist_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "racist_queries = [\"ching chong\",'ching','chong', 'chink', 'chingchong', \"kung flu\",'kung','fu', \"kung fu flu\", \"ching chong virus\",'coronavirus', 'corona virus', 'covid19', 'covid 19']\n",
    "s_racist_tweets_tokens = racist_tokens\n",
    "words_to_remove= stopwords.words('english')+racist_queries\n",
    "for tweet in list(s_racist_tweets_tokens):\n",
    "    if tweet in words_to_remove:\n",
    "        s_racist_tweets_tokens.remove(tweet)\n",
    "\n",
    "s_racist_tweets_tokens = [x for x in s_racist_tweets_tokens if not x.startswith('https')]\n",
    "racist_tweets_wfreq = Counter(s_racist_tweets_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_racist_bigrams = get_ngram_tokens(s_racist_tweets_tokens,2)\n",
    "s_racist_bigrams_dist = Counter(s_racist_bigrams)\n",
    "s_racist_trigrams = get_ngram_tokens(s_racist_tweets_tokens,3)\n",
    "s_racist_trigrams_dist = Counter(s_racist_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_funny_colls = Counter()\n",
    "tweet_funny_colls.update(collocates(racist_tokens, 'funny',win=[5,5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
